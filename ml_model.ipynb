{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-processing of visual input\n",
        "\n",
        "Images are pre-processed through the application of the histogram of oriented gradients (HOG) algorithm.\n",
        "\n",
        "The Pre-processing algorithm is applied to the images to make them suitable to be encoded into the neural circuits. It is inspired by the computations that take place in the first layers of the visual system of the primates,but it is not a realistic description of them.\n",
        "\n",
        "1. The HOG filter is applied (14x14 cell size, 14x14 block size, 9 orientation bins and 7x7 block stride).\n",
        "\n",
        "2. The size of original images is 28 × 28 pixel.\n",
        "\n",
        "3. Histograms are computed using cells of 14 × 14 size that are applied on the images using a striding step of 7 pixels, resulting in 9 histograms per image.\n",
        "\n",
        "4. Each cell provides an histogram with 9 bins, each bin assuming a real value.\n",
        "\n",
        "5. 4 values binning: Each bin is then transformed into a vector of 4 mutually exclusive truth values. The coding of each input feature from acontinuous domain of possible intensities, e.g from the (0.0,1.0) range of possible values,into the discrete activity of subsets of individual thalamic neurons should preserve, at least partially, a distance-like notion (UltraLow:[0; 0.25), MediumLow:[0.25; 0.50), MediumHigh:[0.50; 0.75), High:[0.75,1.0]).\n",
        "\n",
        "In summary, each image has been transformed in a vector of 324 binary features.\n",
        "\n",
        "The resulting visual input is presented to the network through the thalamus, where each feature provides a binary input to a different thalamic cell.\n",
        "\n",
        "HOG documentation: https://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.hog"
      ],
      "metadata": {
        "id": "F1IpLdEe1-Cp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from skimage.feature import hog\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "J7XI8NMfzMP9"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist # 28 x 28 pixels\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "id": "zDM-9Ddy4V0n"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample_image = x_train[0]"
      ],
      "metadata": {
        "id": "_8ljp868rvCN"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HOG transformation"
      ],
      "metadata": {
        "id": "aHIUSmMKqsjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Corrected function for HOG transformation with specific stride\n",
        "def hog_transformation(image, stride=7):\n",
        "    feature_vectors = []\n",
        "\n",
        "    # Size of the image, cells, and blocks\n",
        "    height, width = image.shape\n",
        "    cell_size = block_size = 14\n",
        "\n",
        "    # Iterate over the image with the given stride\n",
        "    for y in range(0, height - cell_size + 1, stride):\n",
        "        for x in range(0, width - cell_size + 1, stride):\n",
        "            cell_region = image[y:y+cell_size, x:x+cell_size]\n",
        "\n",
        "            # Calculate HOG features for the cell\n",
        "            fd, hog_image = hog(\n",
        "                cell_region,\n",
        "                orientations=9,\n",
        "                pixels_per_cell=(cell_size, cell_size),\n",
        "                cells_per_block=(1, 1),\n",
        "                visualize=True,\n",
        "                feature_vector=True\n",
        "            )\n",
        "\n",
        "            feature_vectors.append(fd)\n",
        "\n",
        "    # Concatenate all feature vectors\n",
        "    feature_vector = np.concatenate(feature_vectors)\n",
        "\n",
        "    # print(\"Feature vector:\", feature_vector)\n",
        "    # print(\"Feature vector length:\", len(feature_vector))\n",
        "    # print(\"Feature descriptor:\", fd)\n",
        "    # print(\"Feature descriptor length:\", len(fd))\n",
        "\n",
        "    return feature_vector, hog_image\n",
        ""
      ],
      "metadata": {
        "id": "0oiVM__Mq0Bw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fa497dd-d4e8-4a52-d05f-c158208656ce"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.41049011, 0.        , 0.19546443, 0.41789258, 0.41789258,\n",
              "        0.21716949, 0.39357862, 0.41789258, 0.2594652 , 0.38777022,\n",
              "        0.22175656, 0.28084969, 0.38777022, 0.38777022, 0.37856621,\n",
              "        0.35460294, 0.38777022, 0.03780066, 0.36014781, 0.30524331,\n",
              "        0.4210823 , 0.4210823 , 0.4210823 , 0.41133185, 0.25572006,\n",
              "        0.        , 0.1029685 , 0.43937762, 0.        , 0.18784964,\n",
              "        0.08509439, 0.43937762, 0.26329971, 0.43937762, 0.43937762,\n",
              "        0.34049073, 0.37769285, 0.27588264, 0.22756847, 0.37769285,\n",
              "        0.37769285, 0.37769285, 0.37769285, 0.37769285, 0.12723999,\n",
              "        0.40453902, 0.31073204, 0.        , 0.40453902, 0.40453902,\n",
              "        0.40453902, 0.40453902, 0.        , 0.29186725, 0.1338323 ,\n",
              "        0.10788827, 0.56087562, 0.56087562, 0.56087562, 0.        ,\n",
              "        0.10235298, 0.        , 0.12739124, 0.43866571, 0.08929545,\n",
              "        0.43866571, 0.43866571, 0.43866571, 0.14356505, 0.43866571,\n",
              "        0.        , 0.09631918, 0.39509041, 0.16320965, 0.39509041,\n",
              "        0.39509041, 0.39509041, 0.19179165, 0.39509041, 0.        ,\n",
              "        0.39509041]),\n",
              " array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "          0.        ,  0.        ,  0.        ,  0.        ],\n",
              "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "          4.06511211,  0.        ,  0.        ,  9.71656704,  0.        ,\n",
              "          1.65599537,  0.        ,  0.        ,  0.        ],\n",
              "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "          4.06511211,  0.        ,  0.        ,  9.71656704,  1.65599537,\n",
              "          0.        ,  0.        ,  0.        ,  0.        ],\n",
              "        [ 0.        ,  0.        ,  9.43201923,  0.        ,  0.        ,\n",
              "          0.        ,  4.06511211,  9.71656704,  0.        ,  1.65599537,\n",
              "          0.        ,  6.35374594,  0.        ,  0.        ],\n",
              "        [ 0.        ,  1.9460007 ,  1.9460007 ,  9.43201923,  0.        ,\n",
              "          0.        ,  4.06511211,  9.71656704,  1.65599537,  0.        ,\n",
              "          6.35374594,  8.62835312,  8.62835312,  0.        ],\n",
              "        [ 0.        ,  0.        ,  0.        ,  1.9460007 , 11.37801993,\n",
              "          9.43201923,  4.06511211,  9.71656704,  8.00974131, 14.98209906,\n",
              "          8.62835312,  0.        ,  0.        ,  0.        ],\n",
              "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "          1.9460007 , 15.44313204, 26.35466146,  8.62835312,  0.        ,\n",
              "          0.        ,  0.        ,  0.        ,  0.        ],\n",
              "        [ 0.        ,  8.97250271,  8.97250271,  8.97250271,  8.97250271,\n",
              "         17.60085583, 33.6711688 , 26.07163012, 10.9185034 ,  8.97250271,\n",
              "          8.97250271,  8.97250271,  8.97250271,  8.97250271],\n",
              "        [ 0.        ,  0.        ,  0.        ,  8.62835312, 14.98209906,\n",
              "          6.35374594, 11.37256241,  4.06511211,  9.43201923, 11.37801993,\n",
              "          1.9460007 ,  0.        ,  0.        ,  0.        ],\n",
              "        [ 0.        ,  8.62835312,  8.62835312,  6.35374594,  0.        ,\n",
              "          0.        , 11.37256241,  4.06511211,  0.        ,  0.        ,\n",
              "          9.43201923,  1.9460007 ,  1.9460007 ,  0.        ],\n",
              "        [ 0.        ,  0.        ,  6.35374594,  0.        ,  0.        ,\n",
              "          1.65599537,  9.71656704,  4.06511211,  0.        ,  0.        ,\n",
              "          0.        ,  9.43201923,  0.        ,  0.        ],\n",
              "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         11.37256241,  0.        ,  0.        ,  4.06511211,  0.        ,\n",
              "          0.        ,  0.        ,  0.        ,  0.        ],\n",
              "        [ 0.        ,  0.        ,  0.        ,  0.        ,  1.65599537,\n",
              "          9.71656704,  0.        ,  0.        ,  4.06511211,  0.        ,\n",
              "          0.        ,  0.        ,  0.        ,  0.        ],\n",
              "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "          0.        ,  0.        ,  0.        ,  0.        ]]))"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def bin_feature_values(feature_vector):\n",
        "    '''\n",
        "    Binning the feature values into four categories\n",
        "    '''\n",
        "\n",
        "    binned_vector = []\n",
        "\n",
        "    for value in feature_vector:\n",
        "        if value < 0.25:\n",
        "            binned_vector.append([1, 0, 0, 0])  # UltraLow\n",
        "        elif value < 0.50:\n",
        "            binned_vector.append([0, 1, 0, 0])  # MediumLow\n",
        "        elif value < 0.75:\n",
        "            binned_vector.append([0, 0, 1, 0])  # MediumHigh\n",
        "        else:\n",
        "            binned_vector.append([0, 0, 0, 1])  # High\n",
        "\n",
        "    return np.array(binned_vector).flatten()    # flattening preserves properties but in 1-dimension array\n"
      ],
      "metadata": {
        "id": "JddDyXqXvI29"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to plot the original image and its HOG representation side by side\n",
        "def plot_images(original_image, hog_image):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(original_image, cmap='gray')\n",
        "    plt.title('Original Image')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(hog_image, cmap='gray')\n",
        "    plt.title('HOG Image')\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "ILezOUmcq0gw"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_and_store(x):\n",
        "  '''\n",
        "  Process and store binned features vectors for training data\n",
        "  '''\n",
        "\n",
        "  binned_features = []\n",
        "\n",
        "  for image in x:\n",
        "      feature_vector, hog_image = hog_transformation(image) # Apply HOG transformation to each image\n",
        "\n",
        "      binned_feature = bin_feature_values(feature_vector)   # Bin the feature vector values\n",
        "\n",
        "      binned_features.append(binned_feature)                # Store the binned feature vector\n",
        "\n",
        "      # plot_images(image, hog_image)                       # Plot original and HOG transformed images\n",
        "\n",
        "  binned_features = np.array(binned_features)               # Convert list to numpy array\n",
        "\n",
        "  return binned_features\n",
        "\n",
        "feature_x_train = process_and_store(x_train).copy()\n",
        "feature_x_test = process_and_store(x_test).copy()\n",
        "\n",
        "# print(feature_x_train)\n",
        "# print(feature_x_test)\n",
        "\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "S7Wsigy2yFEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning Model"
      ],
      "metadata": {
        "id": "LcYqmDmvk1Wz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "LqnE95ZEk6Ab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "ZJo3UD_nk5iB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model implementation"
      ],
      "metadata": {
        "id": "j3QRH_cNrZNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a simple CNN model\n",
        "input_shape = (324,)\n",
        "\n",
        "model = models.Sequential([                 # Not original model\n",
        "    layers.Input(shape=input_shape),\n",
        "    layers.Dense(128, activation='relu'),   # You can adjust the number of neurons\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')  # 10 classes for MNIST\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Incremental training\n",
        "batch_size = 25\n",
        "steps = len(feature_x_train) // batch_size\n",
        "\n",
        "for step in range(steps):\n",
        "    start = step * batch_size\n",
        "    end = start + batch_size\n",
        "    x_batch, y_batch = feature_x_train[start:end], y_train[start:end]\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(x_batch, y_batch, epochs=1, verbose=0)\n",
        "\n",
        "    # Evaluate the model\n",
        "    loss, accuracy = model.evaluate(feature_x_test, y_test, verbose=0)\n",
        "    print(f\"After training step {step + 1}, accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# 2400 steps, first trial: 'After training step 2400, accuracy: 0.8874'"
      ],
      "metadata": {
        "id": "EDiNFdstq0rr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5b7d69e-c0d1-41fa-8a04-f4853bc167b6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After training step 1, accuracy: 0.1240\n",
            "After training step 2, accuracy: 0.1532\n",
            "After training step 3, accuracy: 0.1864\n",
            "After training step 4, accuracy: 0.2123\n",
            "After training step 5, accuracy: 0.2386\n",
            "After training step 6, accuracy: 0.2585\n",
            "After training step 7, accuracy: 0.2745\n",
            "After training step 8, accuracy: 0.2909\n",
            "After training step 9, accuracy: 0.3104\n",
            "After training step 10, accuracy: 0.3288\n",
            "After training step 11, accuracy: 0.3502\n",
            "After training step 12, accuracy: 0.3717\n",
            "After training step 13, accuracy: 0.3927\n",
            "After training step 14, accuracy: 0.4104\n",
            "After training step 15, accuracy: 0.4271\n",
            "After training step 16, accuracy: 0.4363\n",
            "After training step 17, accuracy: 0.4405\n",
            "After training step 18, accuracy: 0.4471\n",
            "After training step 19, accuracy: 0.4557\n",
            "After training step 20, accuracy: 0.4666\n",
            "After training step 21, accuracy: 0.4729\n",
            "After training step 22, accuracy: 0.4834\n",
            "After training step 23, accuracy: 0.4951\n",
            "After training step 24, accuracy: 0.5111\n",
            "After training step 25, accuracy: 0.5188\n",
            "After training step 26, accuracy: 0.5309\n",
            "After training step 27, accuracy: 0.5465\n",
            "After training step 28, accuracy: 0.5635\n",
            "After training step 29, accuracy: 0.5768\n",
            "After training step 30, accuracy: 0.5828\n",
            "After training step 31, accuracy: 0.5963\n",
            "After training step 32, accuracy: 0.6052\n",
            "After training step 33, accuracy: 0.6145\n",
            "After training step 34, accuracy: 0.6227\n",
            "After training step 35, accuracy: 0.6336\n",
            "After training step 36, accuracy: 0.6454\n",
            "After training step 37, accuracy: 0.6534\n",
            "After training step 38, accuracy: 0.6547\n",
            "After training step 39, accuracy: 0.6515\n",
            "After training step 40, accuracy: 0.6450\n",
            "After training step 41, accuracy: 0.6396\n",
            "After training step 42, accuracy: 0.6374\n",
            "After training step 43, accuracy: 0.6447\n",
            "After training step 44, accuracy: 0.6579\n",
            "After training step 45, accuracy: 0.6772\n",
            "After training step 46, accuracy: 0.6902\n",
            "After training step 47, accuracy: 0.7027\n",
            "After training step 48, accuracy: 0.7111\n",
            "After training step 49, accuracy: 0.7189\n",
            "After training step 50, accuracy: 0.7164\n",
            "After training step 51, accuracy: 0.7135\n",
            "After training step 52, accuracy: 0.7064\n",
            "After training step 53, accuracy: 0.7061\n",
            "After training step 54, accuracy: 0.7053\n",
            "After training step 55, accuracy: 0.7099\n",
            "After training step 56, accuracy: 0.7130\n",
            "After training step 57, accuracy: 0.7148\n",
            "After training step 58, accuracy: 0.7164\n",
            "After training step 59, accuracy: 0.7240\n",
            "After training step 60, accuracy: 0.7319\n",
            "After training step 61, accuracy: 0.7453\n",
            "After training step 62, accuracy: 0.7485\n",
            "After training step 63, accuracy: 0.7471\n",
            "After training step 64, accuracy: 0.7451\n",
            "After training step 65, accuracy: 0.7473\n",
            "After training step 66, accuracy: 0.7504\n",
            "After training step 67, accuracy: 0.7554\n",
            "After training step 68, accuracy: 0.7579\n",
            "After training step 69, accuracy: 0.7550\n",
            "After training step 70, accuracy: 0.7506\n",
            "After training step 71, accuracy: 0.7498\n",
            "After training step 72, accuracy: 0.7538\n",
            "After training step 73, accuracy: 0.7613\n",
            "After training step 74, accuracy: 0.7694\n",
            "After training step 75, accuracy: 0.7801\n",
            "After training step 76, accuracy: 0.7843\n",
            "After training step 77, accuracy: 0.7880\n",
            "After training step 78, accuracy: 0.7879\n",
            "After training step 79, accuracy: 0.7862\n",
            "After training step 80, accuracy: 0.7871\n",
            "After training step 81, accuracy: 0.7849\n",
            "After training step 82, accuracy: 0.7846\n",
            "After training step 83, accuracy: 0.7847\n",
            "After training step 84, accuracy: 0.7819\n",
            "After training step 85, accuracy: 0.7818\n",
            "After training step 86, accuracy: 0.7809\n",
            "After training step 87, accuracy: 0.7808\n",
            "After training step 88, accuracy: 0.7839\n",
            "After training step 89, accuracy: 0.7872\n",
            "After training step 90, accuracy: 0.7898\n",
            "After training step 91, accuracy: 0.7959\n",
            "After training step 92, accuracy: 0.8018\n",
            "After training step 93, accuracy: 0.8055\n",
            "After training step 94, accuracy: 0.8038\n",
            "After training step 95, accuracy: 0.8015\n",
            "After training step 96, accuracy: 0.8010\n",
            "After training step 97, accuracy: 0.8033\n",
            "After training step 98, accuracy: 0.7977\n",
            "After training step 99, accuracy: 0.7950\n",
            "After training step 100, accuracy: 0.7930\n",
            "After training step 101, accuracy: 0.7946\n",
            "After training step 102, accuracy: 0.7917\n",
            "After training step 103, accuracy: 0.7867\n",
            "After training step 104, accuracy: 0.7806\n",
            "After training step 105, accuracy: 0.7760\n",
            "After training step 106, accuracy: 0.7731\n",
            "After training step 107, accuracy: 0.7729\n",
            "After training step 108, accuracy: 0.7802\n",
            "After training step 109, accuracy: 0.7894\n",
            "After training step 110, accuracy: 0.7986\n",
            "After training step 111, accuracy: 0.7992\n",
            "After training step 112, accuracy: 0.7949\n",
            "After training step 113, accuracy: 0.7849\n",
            "After training step 114, accuracy: 0.7741\n",
            "After training step 115, accuracy: 0.7723\n",
            "After training step 116, accuracy: 0.7746\n",
            "After training step 117, accuracy: 0.7802\n",
            "After training step 118, accuracy: 0.7913\n",
            "After training step 119, accuracy: 0.8033\n",
            "After training step 120, accuracy: 0.8114\n",
            "After training step 121, accuracy: 0.8187\n",
            "After training step 122, accuracy: 0.8217\n",
            "After training step 123, accuracy: 0.8213\n",
            "After training step 124, accuracy: 0.8187\n",
            "After training step 125, accuracy: 0.8188\n",
            "After training step 126, accuracy: 0.8199\n",
            "After training step 127, accuracy: 0.8228\n",
            "After training step 128, accuracy: 0.8222\n",
            "After training step 129, accuracy: 0.8227\n",
            "After training step 130, accuracy: 0.8199\n",
            "After training step 131, accuracy: 0.8160\n",
            "After training step 132, accuracy: 0.8141\n",
            "After training step 133, accuracy: 0.8084\n",
            "After training step 134, accuracy: 0.8065\n",
            "After training step 135, accuracy: 0.8060\n",
            "After training step 136, accuracy: 0.8071\n",
            "After training step 137, accuracy: 0.8069\n",
            "After training step 138, accuracy: 0.8074\n",
            "After training step 139, accuracy: 0.8079\n",
            "After training step 140, accuracy: 0.8104\n",
            "After training step 141, accuracy: 0.8133\n",
            "After training step 142, accuracy: 0.8169\n",
            "After training step 143, accuracy: 0.8172\n",
            "After training step 144, accuracy: 0.8137\n",
            "After training step 145, accuracy: 0.8070\n",
            "After training step 146, accuracy: 0.8033\n",
            "After training step 147, accuracy: 0.8022\n",
            "After training step 148, accuracy: 0.8052\n",
            "After training step 149, accuracy: 0.8070\n",
            "After training step 150, accuracy: 0.8102\n",
            "After training step 151, accuracy: 0.8159\n",
            "After training step 152, accuracy: 0.8219\n",
            "After training step 153, accuracy: 0.8291\n",
            "After training step 154, accuracy: 0.8340\n",
            "After training step 155, accuracy: 0.8348\n",
            "After training step 156, accuracy: 0.8330\n",
            "After training step 157, accuracy: 0.8286\n",
            "After training step 158, accuracy: 0.8282\n",
            "After training step 159, accuracy: 0.8279\n",
            "After training step 160, accuracy: 0.8292\n",
            "After training step 161, accuracy: 0.8303\n",
            "After training step 162, accuracy: 0.8295\n",
            "After training step 163, accuracy: 0.8302\n",
            "After training step 164, accuracy: 0.8289\n",
            "After training step 165, accuracy: 0.8280\n",
            "After training step 166, accuracy: 0.8265\n",
            "After training step 167, accuracy: 0.8237\n",
            "After training step 168, accuracy: 0.8237\n",
            "After training step 169, accuracy: 0.8265\n",
            "After training step 170, accuracy: 0.8266\n",
            "After training step 171, accuracy: 0.8288\n",
            "After training step 172, accuracy: 0.8319\n",
            "After training step 173, accuracy: 0.8358\n",
            "After training step 174, accuracy: 0.8356\n",
            "After training step 175, accuracy: 0.8257\n",
            "After training step 176, accuracy: 0.8195\n",
            "After training step 177, accuracy: 0.8131\n",
            "After training step 178, accuracy: 0.8125\n",
            "After training step 179, accuracy: 0.8184\n",
            "After training step 180, accuracy: 0.8258\n",
            "After training step 181, accuracy: 0.8302\n",
            "After training step 182, accuracy: 0.8349\n",
            "After training step 183, accuracy: 0.8379\n",
            "After training step 184, accuracy: 0.8369\n",
            "After training step 185, accuracy: 0.8330\n",
            "After training step 186, accuracy: 0.8297\n",
            "After training step 187, accuracy: 0.8241\n",
            "After training step 188, accuracy: 0.8260\n",
            "After training step 189, accuracy: 0.8266\n",
            "After training step 190, accuracy: 0.8314\n",
            "After training step 191, accuracy: 0.8346\n",
            "After training step 192, accuracy: 0.8357\n",
            "After training step 193, accuracy: 0.8361\n",
            "After training step 194, accuracy: 0.8357\n",
            "After training step 195, accuracy: 0.8317\n",
            "After training step 196, accuracy: 0.8270\n",
            "After training step 197, accuracy: 0.8200\n",
            "After training step 198, accuracy: 0.8129\n",
            "After training step 199, accuracy: 0.8109\n",
            "After training step 200, accuracy: 0.8124\n",
            "After training step 201, accuracy: 0.8119\n",
            "After training step 202, accuracy: 0.8167\n",
            "After training step 203, accuracy: 0.8249\n",
            "After training step 204, accuracy: 0.8421\n",
            "After training step 205, accuracy: 0.8467\n",
            "After training step 206, accuracy: 0.8474\n",
            "After training step 207, accuracy: 0.8309\n",
            "After training step 208, accuracy: 0.8089\n",
            "After training step 209, accuracy: 0.7925\n",
            "After training step 210, accuracy: 0.7873\n",
            "After training step 211, accuracy: 0.7942\n",
            "After training step 212, accuracy: 0.8004\n",
            "After training step 213, accuracy: 0.8066\n",
            "After training step 214, accuracy: 0.8153\n",
            "After training step 215, accuracy: 0.8256\n",
            "After training step 216, accuracy: 0.8299\n",
            "After training step 217, accuracy: 0.8319\n",
            "After training step 218, accuracy: 0.8290\n",
            "After training step 219, accuracy: 0.8242\n",
            "After training step 220, accuracy: 0.8190\n",
            "After training step 221, accuracy: 0.8160\n",
            "After training step 222, accuracy: 0.8127\n",
            "After training step 223, accuracy: 0.8113\n",
            "After training step 224, accuracy: 0.8111\n",
            "After training step 225, accuracy: 0.8152\n",
            "After training step 226, accuracy: 0.8250\n",
            "After training step 227, accuracy: 0.8318\n",
            "After training step 228, accuracy: 0.8374\n",
            "After training step 229, accuracy: 0.8442\n",
            "After training step 230, accuracy: 0.8447\n",
            "After training step 231, accuracy: 0.8434\n",
            "After training step 232, accuracy: 0.8396\n",
            "After training step 233, accuracy: 0.8345\n",
            "After training step 234, accuracy: 0.8257\n",
            "After training step 235, accuracy: 0.8233\n",
            "After training step 236, accuracy: 0.8232\n",
            "After training step 237, accuracy: 0.8241\n",
            "After training step 238, accuracy: 0.8265\n",
            "After training step 239, accuracy: 0.8287\n",
            "After training step 240, accuracy: 0.8288\n",
            "After training step 241, accuracy: 0.8323\n",
            "After training step 242, accuracy: 0.8371\n",
            "After training step 243, accuracy: 0.8381\n",
            "After training step 244, accuracy: 0.8396\n",
            "After training step 245, accuracy: 0.8413\n",
            "After training step 246, accuracy: 0.8406\n",
            "After training step 247, accuracy: 0.8385\n",
            "After training step 248, accuracy: 0.8347\n",
            "After training step 249, accuracy: 0.8327\n",
            "After training step 250, accuracy: 0.8320\n",
            "After training step 251, accuracy: 0.8341\n",
            "After training step 252, accuracy: 0.8396\n",
            "After training step 253, accuracy: 0.8419\n",
            "After training step 254, accuracy: 0.8372\n",
            "After training step 255, accuracy: 0.8287\n",
            "After training step 256, accuracy: 0.8211\n",
            "After training step 257, accuracy: 0.8108\n",
            "After training step 258, accuracy: 0.8003\n",
            "After training step 259, accuracy: 0.7993\n",
            "After training step 260, accuracy: 0.8017\n",
            "After training step 261, accuracy: 0.8093\n",
            "After training step 262, accuracy: 0.8201\n",
            "After training step 263, accuracy: 0.8301\n",
            "After training step 264, accuracy: 0.8414\n",
            "After training step 265, accuracy: 0.8477\n",
            "After training step 266, accuracy: 0.8509\n",
            "After training step 267, accuracy: 0.8507\n",
            "After training step 268, accuracy: 0.8496\n",
            "After training step 269, accuracy: 0.8515\n",
            "After training step 270, accuracy: 0.8518\n",
            "After training step 271, accuracy: 0.8529\n",
            "After training step 272, accuracy: 0.8536\n",
            "After training step 273, accuracy: 0.8556\n",
            "After training step 274, accuracy: 0.8567\n",
            "After training step 275, accuracy: 0.8562\n",
            "After training step 276, accuracy: 0.8527\n",
            "After training step 277, accuracy: 0.8451\n",
            "After training step 278, accuracy: 0.8413\n",
            "After training step 279, accuracy: 0.8420\n",
            "After training step 280, accuracy: 0.8458\n",
            "After training step 281, accuracy: 0.8486\n",
            "After training step 282, accuracy: 0.8528\n",
            "After training step 283, accuracy: 0.8545\n",
            "After training step 284, accuracy: 0.8569\n",
            "After training step 285, accuracy: 0.8606\n",
            "After training step 286, accuracy: 0.8595\n",
            "After training step 287, accuracy: 0.8596\n",
            "After training step 288, accuracy: 0.8581\n",
            "After training step 289, accuracy: 0.8542\n",
            "After training step 290, accuracy: 0.8507\n",
            "After training step 291, accuracy: 0.8470\n",
            "After training step 292, accuracy: 0.8435\n",
            "After training step 293, accuracy: 0.8411\n",
            "After training step 294, accuracy: 0.8400\n",
            "After training step 295, accuracy: 0.8395\n",
            "After training step 296, accuracy: 0.8378\n",
            "After training step 297, accuracy: 0.8352\n",
            "After training step 298, accuracy: 0.8356\n",
            "After training step 299, accuracy: 0.8392\n",
            "After training step 300, accuracy: 0.8445\n",
            "After training step 301, accuracy: 0.8506\n",
            "After training step 302, accuracy: 0.8548\n",
            "After training step 303, accuracy: 0.8557\n",
            "After training step 304, accuracy: 0.8590\n",
            "After training step 305, accuracy: 0.8607\n",
            "After training step 306, accuracy: 0.8600\n",
            "After training step 307, accuracy: 0.8580\n",
            "After training step 308, accuracy: 0.8560\n",
            "After training step 309, accuracy: 0.8570\n",
            "After training step 310, accuracy: 0.8571\n",
            "After training step 311, accuracy: 0.8567\n",
            "After training step 312, accuracy: 0.8584\n",
            "After training step 313, accuracy: 0.8577\n",
            "After training step 314, accuracy: 0.8563\n",
            "After training step 315, accuracy: 0.8539\n",
            "After training step 316, accuracy: 0.8511\n",
            "After training step 317, accuracy: 0.8487\n",
            "After training step 318, accuracy: 0.8445\n",
            "After training step 319, accuracy: 0.8418\n",
            "After training step 320, accuracy: 0.8411\n",
            "After training step 321, accuracy: 0.8422\n",
            "After training step 322, accuracy: 0.8455\n",
            "After training step 323, accuracy: 0.8512\n",
            "After training step 324, accuracy: 0.8539\n",
            "After training step 325, accuracy: 0.8560\n",
            "After training step 326, accuracy: 0.8539\n",
            "After training step 327, accuracy: 0.8519\n",
            "After training step 328, accuracy: 0.8477\n",
            "After training step 329, accuracy: 0.8477\n",
            "After training step 330, accuracy: 0.8475\n",
            "After training step 331, accuracy: 0.8509\n",
            "After training step 332, accuracy: 0.8507\n",
            "After training step 333, accuracy: 0.8537\n",
            "After training step 334, accuracy: 0.8577\n",
            "After training step 335, accuracy: 0.8597\n",
            "After training step 336, accuracy: 0.8608\n",
            "After training step 337, accuracy: 0.8615\n",
            "After training step 338, accuracy: 0.8626\n",
            "After training step 339, accuracy: 0.8593\n",
            "After training step 340, accuracy: 0.8571\n",
            "After training step 341, accuracy: 0.8559\n",
            "After training step 342, accuracy: 0.8532\n",
            "After training step 343, accuracy: 0.8517\n",
            "After training step 344, accuracy: 0.8509\n",
            "After training step 345, accuracy: 0.8485\n",
            "After training step 346, accuracy: 0.8497\n",
            "After training step 347, accuracy: 0.8523\n",
            "After training step 348, accuracy: 0.8569\n",
            "After training step 349, accuracy: 0.8631\n",
            "After training step 350, accuracy: 0.8623\n",
            "After training step 351, accuracy: 0.8590\n",
            "After training step 352, accuracy: 0.8547\n",
            "After training step 353, accuracy: 0.8498\n",
            "After training step 354, accuracy: 0.8477\n",
            "After training step 355, accuracy: 0.8453\n",
            "After training step 356, accuracy: 0.8478\n",
            "After training step 357, accuracy: 0.8483\n",
            "After training step 358, accuracy: 0.8454\n",
            "After training step 359, accuracy: 0.8441\n",
            "After training step 360, accuracy: 0.8431\n",
            "After training step 361, accuracy: 0.8398\n",
            "After training step 362, accuracy: 0.8377\n",
            "After training step 363, accuracy: 0.8393\n",
            "After training step 364, accuracy: 0.8405\n",
            "After training step 365, accuracy: 0.8411\n",
            "After training step 366, accuracy: 0.8406\n",
            "After training step 367, accuracy: 0.8415\n",
            "After training step 368, accuracy: 0.8411\n",
            "After training step 369, accuracy: 0.8413\n",
            "After training step 370, accuracy: 0.8455\n",
            "After training step 371, accuracy: 0.8495\n",
            "After training step 372, accuracy: 0.8531\n",
            "After training step 373, accuracy: 0.8550\n",
            "After training step 374, accuracy: 0.8601\n",
            "After training step 375, accuracy: 0.8615\n",
            "After training step 376, accuracy: 0.8635\n",
            "After training step 377, accuracy: 0.8641\n",
            "After training step 378, accuracy: 0.8576\n",
            "After training step 379, accuracy: 0.8491\n",
            "After training step 380, accuracy: 0.8457\n",
            "After training step 381, accuracy: 0.8446\n",
            "After training step 382, accuracy: 0.8478\n",
            "After training step 383, accuracy: 0.8569\n",
            "After training step 384, accuracy: 0.8632\n",
            "After training step 385, accuracy: 0.8658\n",
            "After training step 386, accuracy: 0.8666\n",
            "After training step 387, accuracy: 0.8624\n",
            "After training step 388, accuracy: 0.8554\n",
            "After training step 389, accuracy: 0.8480\n",
            "After training step 390, accuracy: 0.8364\n",
            "After training step 391, accuracy: 0.8291\n",
            "After training step 392, accuracy: 0.8191\n",
            "After training step 393, accuracy: 0.8156\n",
            "After training step 394, accuracy: 0.8169\n",
            "After training step 395, accuracy: 0.8182\n",
            "After training step 396, accuracy: 0.8226\n",
            "After training step 397, accuracy: 0.8273\n",
            "After training step 398, accuracy: 0.8309\n",
            "After training step 399, accuracy: 0.8342\n",
            "After training step 400, accuracy: 0.8372\n",
            "After training step 401, accuracy: 0.8414\n",
            "After training step 402, accuracy: 0.8472\n",
            "After training step 403, accuracy: 0.8478\n",
            "After training step 404, accuracy: 0.8486\n",
            "After training step 405, accuracy: 0.8474\n",
            "After training step 406, accuracy: 0.8473\n",
            "After training step 407, accuracy: 0.8467\n",
            "After training step 408, accuracy: 0.8463\n",
            "After training step 409, accuracy: 0.8492\n",
            "After training step 410, accuracy: 0.8564\n",
            "After training step 411, accuracy: 0.8595\n",
            "After training step 412, accuracy: 0.8604\n",
            "After training step 413, accuracy: 0.8586\n",
            "After training step 414, accuracy: 0.8510\n",
            "After training step 415, accuracy: 0.8459\n",
            "After training step 416, accuracy: 0.8412\n",
            "After training step 417, accuracy: 0.8379\n",
            "After training step 418, accuracy: 0.8384\n",
            "After training step 419, accuracy: 0.8405\n",
            "After training step 420, accuracy: 0.8406\n",
            "After training step 421, accuracy: 0.8437\n",
            "After training step 422, accuracy: 0.8488\n",
            "After training step 423, accuracy: 0.8512\n",
            "After training step 424, accuracy: 0.8556\n",
            "After training step 425, accuracy: 0.8585\n",
            "After training step 426, accuracy: 0.8597\n",
            "After training step 427, accuracy: 0.8653\n",
            "After training step 428, accuracy: 0.8678\n",
            "After training step 429, accuracy: 0.8697\n",
            "After training step 430, accuracy: 0.8690\n",
            "After training step 431, accuracy: 0.8679\n",
            "After training step 432, accuracy: 0.8628\n",
            "After training step 433, accuracy: 0.8580\n",
            "After training step 434, accuracy: 0.8531\n",
            "After training step 435, accuracy: 0.8507\n",
            "After training step 436, accuracy: 0.8494\n",
            "After training step 437, accuracy: 0.8465\n",
            "After training step 438, accuracy: 0.8467\n",
            "After training step 439, accuracy: 0.8480\n",
            "After training step 440, accuracy: 0.8493\n",
            "After training step 441, accuracy: 0.8539\n",
            "After training step 442, accuracy: 0.8550\n",
            "After training step 443, accuracy: 0.8599\n",
            "After training step 444, accuracy: 0.8620\n",
            "After training step 445, accuracy: 0.8649\n",
            "After training step 446, accuracy: 0.8658\n",
            "After training step 447, accuracy: 0.8636\n",
            "After training step 448, accuracy: 0.8677\n",
            "After training step 449, accuracy: 0.8686\n",
            "After training step 450, accuracy: 0.8655\n",
            "After training step 451, accuracy: 0.8613\n",
            "After training step 452, accuracy: 0.8583\n",
            "After training step 453, accuracy: 0.8537\n",
            "After training step 454, accuracy: 0.8511\n",
            "After training step 455, accuracy: 0.8491\n",
            "After training step 456, accuracy: 0.8536\n",
            "After training step 457, accuracy: 0.8552\n",
            "After training step 458, accuracy: 0.8591\n",
            "After training step 459, accuracy: 0.8631\n",
            "After training step 460, accuracy: 0.8667\n",
            "After training step 461, accuracy: 0.8668\n",
            "After training step 462, accuracy: 0.8673\n",
            "After training step 463, accuracy: 0.8654\n",
            "After training step 464, accuracy: 0.8613\n",
            "After training step 465, accuracy: 0.8581\n",
            "After training step 466, accuracy: 0.8568\n",
            "After training step 467, accuracy: 0.8549\n",
            "After training step 468, accuracy: 0.8516\n",
            "After training step 469, accuracy: 0.8500\n",
            "After training step 470, accuracy: 0.8507\n",
            "After training step 471, accuracy: 0.8513\n",
            "After training step 472, accuracy: 0.8522\n",
            "After training step 473, accuracy: 0.8539\n",
            "After training step 474, accuracy: 0.8538\n",
            "After training step 475, accuracy: 0.8549\n",
            "After training step 476, accuracy: 0.8569\n",
            "After training step 477, accuracy: 0.8550\n",
            "After training step 478, accuracy: 0.8541\n",
            "After training step 479, accuracy: 0.8534\n",
            "After training step 480, accuracy: 0.8538\n",
            "After training step 481, accuracy: 0.8536\n",
            "After training step 482, accuracy: 0.8538\n",
            "After training step 483, accuracy: 0.8569\n",
            "After training step 484, accuracy: 0.8590\n",
            "After training step 485, accuracy: 0.8611\n",
            "After training step 486, accuracy: 0.8627\n",
            "After training step 487, accuracy: 0.8640\n",
            "After training step 488, accuracy: 0.8631\n",
            "After training step 489, accuracy: 0.8616\n",
            "After training step 490, accuracy: 0.8594\n",
            "After training step 491, accuracy: 0.8543\n",
            "After training step 492, accuracy: 0.8525\n",
            "After training step 493, accuracy: 0.8531\n",
            "After training step 494, accuracy: 0.8509\n",
            "After training step 495, accuracy: 0.8501\n",
            "After training step 496, accuracy: 0.8494\n",
            "After training step 497, accuracy: 0.8520\n",
            "After training step 498, accuracy: 0.8527\n",
            "After training step 499, accuracy: 0.8565\n",
            "After training step 500, accuracy: 0.8608\n",
            "After training step 501, accuracy: 0.8662\n",
            "After training step 502, accuracy: 0.8700\n",
            "After training step 503, accuracy: 0.8715\n",
            "After training step 504, accuracy: 0.8736\n",
            "After training step 505, accuracy: 0.8757\n",
            "After training step 506, accuracy: 0.8732\n",
            "After training step 507, accuracy: 0.8718\n",
            "After training step 508, accuracy: 0.8693\n",
            "After training step 509, accuracy: 0.8656\n",
            "After training step 510, accuracy: 0.8650\n",
            "After training step 511, accuracy: 0.8641\n",
            "After training step 512, accuracy: 0.8644\n",
            "After training step 513, accuracy: 0.8657\n",
            "After training step 514, accuracy: 0.8673\n",
            "After training step 515, accuracy: 0.8686\n",
            "After training step 516, accuracy: 0.8686\n",
            "After training step 517, accuracy: 0.8714\n",
            "After training step 518, accuracy: 0.8749\n",
            "After training step 519, accuracy: 0.8751\n",
            "After training step 520, accuracy: 0.8724\n",
            "After training step 521, accuracy: 0.8718\n",
            "After training step 522, accuracy: 0.8711\n",
            "After training step 523, accuracy: 0.8708\n",
            "After training step 524, accuracy: 0.8679\n",
            "After training step 525, accuracy: 0.8654\n",
            "After training step 526, accuracy: 0.8631\n",
            "After training step 527, accuracy: 0.8600\n",
            "After training step 528, accuracy: 0.8578\n",
            "After training step 529, accuracy: 0.8569\n",
            "After training step 530, accuracy: 0.8558\n",
            "After training step 531, accuracy: 0.8565\n",
            "After training step 532, accuracy: 0.8573\n",
            "After training step 533, accuracy: 0.8601\n",
            "After training step 534, accuracy: 0.8631\n",
            "After training step 535, accuracy: 0.8653\n",
            "After training step 536, accuracy: 0.8658\n",
            "After training step 537, accuracy: 0.8659\n",
            "After training step 538, accuracy: 0.8649\n",
            "After training step 539, accuracy: 0.8629\n",
            "After training step 540, accuracy: 0.8621\n",
            "After training step 541, accuracy: 0.8614\n",
            "After training step 542, accuracy: 0.8627\n",
            "After training step 543, accuracy: 0.8620\n",
            "After training step 544, accuracy: 0.8624\n",
            "After training step 545, accuracy: 0.8626\n",
            "After training step 546, accuracy: 0.8623\n",
            "After training step 547, accuracy: 0.8640\n",
            "After training step 548, accuracy: 0.8648\n",
            "After training step 549, accuracy: 0.8653\n",
            "After training step 550, accuracy: 0.8652\n",
            "After training step 551, accuracy: 0.8667\n",
            "After training step 552, accuracy: 0.8660\n",
            "After training step 553, accuracy: 0.8682\n",
            "After training step 554, accuracy: 0.8687\n",
            "After training step 555, accuracy: 0.8699\n",
            "After training step 556, accuracy: 0.8715\n",
            "After training step 557, accuracy: 0.8723\n",
            "After training step 558, accuracy: 0.8723\n",
            "After training step 559, accuracy: 0.8727\n",
            "After training step 560, accuracy: 0.8713\n",
            "After training step 561, accuracy: 0.8693\n",
            "After training step 562, accuracy: 0.8673\n",
            "After training step 563, accuracy: 0.8651\n",
            "After training step 564, accuracy: 0.8637\n",
            "After training step 565, accuracy: 0.8636\n",
            "After training step 566, accuracy: 0.8665\n",
            "After training step 567, accuracy: 0.8683\n",
            "After training step 568, accuracy: 0.8694\n",
            "After training step 569, accuracy: 0.8713\n",
            "After training step 570, accuracy: 0.8717\n",
            "After training step 571, accuracy: 0.8723\n",
            "After training step 572, accuracy: 0.8751\n",
            "After training step 573, accuracy: 0.8764\n",
            "After training step 574, accuracy: 0.8751\n",
            "After training step 575, accuracy: 0.8752\n",
            "After training step 576, accuracy: 0.8742\n",
            "After training step 577, accuracy: 0.8720\n",
            "After training step 578, accuracy: 0.8685\n",
            "After training step 579, accuracy: 0.8675\n",
            "After training step 580, accuracy: 0.8645\n",
            "After training step 581, accuracy: 0.8626\n",
            "After training step 582, accuracy: 0.8647\n",
            "After training step 583, accuracy: 0.8649\n",
            "After training step 584, accuracy: 0.8663\n",
            "After training step 585, accuracy: 0.8676\n",
            "After training step 586, accuracy: 0.8690\n",
            "After training step 587, accuracy: 0.8680\n",
            "After training step 588, accuracy: 0.8639\n",
            "After training step 589, accuracy: 0.8637\n",
            "After training step 590, accuracy: 0.8643\n",
            "After training step 591, accuracy: 0.8657\n",
            "After training step 592, accuracy: 0.8661\n",
            "After training step 593, accuracy: 0.8633\n",
            "After training step 594, accuracy: 0.8613\n",
            "After training step 595, accuracy: 0.8602\n",
            "After training step 596, accuracy: 0.8606\n",
            "After training step 597, accuracy: 0.8587\n",
            "After training step 598, accuracy: 0.8595\n",
            "After training step 599, accuracy: 0.8593\n",
            "After training step 600, accuracy: 0.8615\n",
            "After training step 601, accuracy: 0.8646\n",
            "After training step 602, accuracy: 0.8658\n",
            "After training step 603, accuracy: 0.8675\n",
            "After training step 604, accuracy: 0.8693\n",
            "After training step 605, accuracy: 0.8689\n",
            "After training step 606, accuracy: 0.8698\n",
            "After training step 607, accuracy: 0.8702\n",
            "After training step 608, accuracy: 0.8730\n",
            "After training step 609, accuracy: 0.8730\n",
            "After training step 610, accuracy: 0.8749\n",
            "After training step 611, accuracy: 0.8757\n",
            "After training step 612, accuracy: 0.8729\n",
            "After training step 613, accuracy: 0.8701\n",
            "After training step 614, accuracy: 0.8683\n",
            "After training step 615, accuracy: 0.8671\n",
            "After training step 616, accuracy: 0.8641\n",
            "After training step 617, accuracy: 0.8605\n",
            "After training step 618, accuracy: 0.8568\n",
            "After training step 619, accuracy: 0.8528\n",
            "After training step 620, accuracy: 0.8507\n",
            "After training step 621, accuracy: 0.8499\n",
            "After training step 622, accuracy: 0.8483\n",
            "After training step 623, accuracy: 0.8512\n",
            "After training step 624, accuracy: 0.8541\n",
            "After training step 625, accuracy: 0.8570\n",
            "After training step 626, accuracy: 0.8596\n",
            "After training step 627, accuracy: 0.8609\n",
            "After training step 628, accuracy: 0.8631\n",
            "After training step 629, accuracy: 0.8635\n",
            "After training step 630, accuracy: 0.8601\n",
            "After training step 631, accuracy: 0.8560\n",
            "After training step 632, accuracy: 0.8551\n",
            "After training step 633, accuracy: 0.8555\n",
            "After training step 634, accuracy: 0.8572\n",
            "After training step 635, accuracy: 0.8605\n",
            "After training step 636, accuracy: 0.8623\n",
            "After training step 637, accuracy: 0.8639\n",
            "After training step 638, accuracy: 0.8646\n",
            "After training step 639, accuracy: 0.8644\n",
            "After training step 640, accuracy: 0.8628\n",
            "After training step 641, accuracy: 0.8612\n",
            "After training step 642, accuracy: 0.8616\n",
            "After training step 643, accuracy: 0.8632\n",
            "After training step 644, accuracy: 0.8648\n",
            "After training step 645, accuracy: 0.8668\n",
            "After training step 646, accuracy: 0.8688\n",
            "After training step 647, accuracy: 0.8718\n",
            "After training step 648, accuracy: 0.8726\n",
            "After training step 649, accuracy: 0.8709\n",
            "After training step 650, accuracy: 0.8698\n",
            "After training step 651, accuracy: 0.8679\n",
            "After training step 652, accuracy: 0.8658\n",
            "After training step 653, accuracy: 0.8649\n",
            "After training step 654, accuracy: 0.8617\n",
            "After training step 655, accuracy: 0.8623\n",
            "After training step 656, accuracy: 0.8645\n",
            "After training step 657, accuracy: 0.8670\n",
            "After training step 658, accuracy: 0.8695\n",
            "After training step 659, accuracy: 0.8715\n",
            "After training step 660, accuracy: 0.8714\n",
            "After training step 661, accuracy: 0.8743\n",
            "After training step 662, accuracy: 0.8747\n",
            "After training step 663, accuracy: 0.8758\n",
            "After training step 664, accuracy: 0.8752\n",
            "After training step 665, accuracy: 0.8714\n",
            "After training step 666, accuracy: 0.8683\n",
            "After training step 667, accuracy: 0.8655\n",
            "After training step 668, accuracy: 0.8641\n",
            "After training step 669, accuracy: 0.8633\n",
            "After training step 670, accuracy: 0.8623\n",
            "After training step 671, accuracy: 0.8625\n",
            "After training step 672, accuracy: 0.8665\n",
            "After training step 673, accuracy: 0.8699\n",
            "After training step 674, accuracy: 0.8715\n",
            "After training step 675, accuracy: 0.8743\n",
            "After training step 676, accuracy: 0.8739\n",
            "After training step 677, accuracy: 0.8744\n",
            "After training step 678, accuracy: 0.8741\n",
            "After training step 679, accuracy: 0.8728\n",
            "After training step 680, accuracy: 0.8728\n",
            "After training step 681, accuracy: 0.8738\n",
            "After training step 682, accuracy: 0.8738\n",
            "After training step 683, accuracy: 0.8749\n",
            "After training step 684, accuracy: 0.8753\n",
            "After training step 685, accuracy: 0.8763\n",
            "After training step 686, accuracy: 0.8752\n",
            "After training step 687, accuracy: 0.8737\n",
            "After training step 688, accuracy: 0.8731\n",
            "After training step 689, accuracy: 0.8712\n",
            "After training step 690, accuracy: 0.8691\n",
            "After training step 691, accuracy: 0.8677\n",
            "After training step 692, accuracy: 0.8655\n",
            "After training step 693, accuracy: 0.8630\n",
            "After training step 694, accuracy: 0.8615\n",
            "After training step 695, accuracy: 0.8607\n",
            "After training step 696, accuracy: 0.8586\n",
            "After training step 697, accuracy: 0.8568\n",
            "After training step 698, accuracy: 0.8570\n",
            "After training step 699, accuracy: 0.8570\n",
            "After training step 700, accuracy: 0.8593\n",
            "After training step 701, accuracy: 0.8631\n",
            "After training step 702, accuracy: 0.8687\n",
            "After training step 703, accuracy: 0.8706\n",
            "After training step 704, accuracy: 0.8718\n",
            "After training step 705, accuracy: 0.8694\n",
            "After training step 706, accuracy: 0.8655\n",
            "After training step 707, accuracy: 0.8625\n",
            "After training step 708, accuracy: 0.8591\n",
            "After training step 709, accuracy: 0.8555\n",
            "After training step 710, accuracy: 0.8552\n",
            "After training step 711, accuracy: 0.8560\n",
            "After training step 712, accuracy: 0.8614\n",
            "After training step 713, accuracy: 0.8661\n",
            "After training step 714, accuracy: 0.8701\n",
            "After training step 715, accuracy: 0.8729\n",
            "After training step 716, accuracy: 0.8757\n",
            "After training step 717, accuracy: 0.8767\n",
            "After training step 718, accuracy: 0.8796\n",
            "After training step 719, accuracy: 0.8788\n",
            "After training step 720, accuracy: 0.8769\n",
            "After training step 721, accuracy: 0.8750\n",
            "After training step 722, accuracy: 0.8736\n",
            "After training step 723, accuracy: 0.8696\n",
            "After training step 724, accuracy: 0.8696\n",
            "After training step 725, accuracy: 0.8693\n",
            "After training step 726, accuracy: 0.8685\n",
            "After training step 727, accuracy: 0.8677\n",
            "After training step 728, accuracy: 0.8671\n",
            "After training step 729, accuracy: 0.8706\n",
            "After training step 730, accuracy: 0.8719\n",
            "After training step 731, accuracy: 0.8726\n",
            "After training step 732, accuracy: 0.8730\n",
            "After training step 733, accuracy: 0.8730\n",
            "After training step 734, accuracy: 0.8745\n",
            "After training step 735, accuracy: 0.8757\n",
            "After training step 736, accuracy: 0.8748\n",
            "After training step 737, accuracy: 0.8764\n",
            "After training step 738, accuracy: 0.8767\n",
            "After training step 739, accuracy: 0.8793\n",
            "After training step 740, accuracy: 0.8792\n",
            "After training step 741, accuracy: 0.8759\n",
            "After training step 742, accuracy: 0.8739\n",
            "After training step 743, accuracy: 0.8735\n",
            "After training step 744, accuracy: 0.8714\n",
            "After training step 745, accuracy: 0.8699\n",
            "After training step 746, accuracy: 0.8696\n",
            "After training step 747, accuracy: 0.8706\n",
            "After training step 748, accuracy: 0.8732\n",
            "After training step 749, accuracy: 0.8754\n",
            "After training step 750, accuracy: 0.8766\n",
            "After training step 751, accuracy: 0.8782\n",
            "After training step 752, accuracy: 0.8796\n",
            "After training step 753, accuracy: 0.8814\n",
            "After training step 754, accuracy: 0.8818\n",
            "After training step 755, accuracy: 0.8812\n",
            "After training step 756, accuracy: 0.8798\n",
            "After training step 757, accuracy: 0.8785\n",
            "After training step 758, accuracy: 0.8772\n",
            "After training step 759, accuracy: 0.8765\n",
            "After training step 760, accuracy: 0.8756\n",
            "After training step 761, accuracy: 0.8757\n",
            "After training step 762, accuracy: 0.8756\n",
            "After training step 763, accuracy: 0.8768\n",
            "After training step 764, accuracy: 0.8741\n",
            "After training step 765, accuracy: 0.8739\n",
            "After training step 766, accuracy: 0.8714\n",
            "After training step 767, accuracy: 0.8681\n",
            "After training step 768, accuracy: 0.8663\n",
            "After training step 769, accuracy: 0.8657\n",
            "After training step 770, accuracy: 0.8650\n",
            "After training step 771, accuracy: 0.8674\n",
            "After training step 772, accuracy: 0.8699\n",
            "After training step 773, accuracy: 0.8725\n",
            "After training step 774, accuracy: 0.8748\n",
            "After training step 775, accuracy: 0.8759\n",
            "After training step 776, accuracy: 0.8774\n",
            "After training step 777, accuracy: 0.8778\n",
            "After training step 778, accuracy: 0.8793\n",
            "After training step 779, accuracy: 0.8800\n",
            "After training step 780, accuracy: 0.8798\n",
            "After training step 781, accuracy: 0.8810\n",
            "After training step 782, accuracy: 0.8840\n",
            "After training step 783, accuracy: 0.8836\n",
            "After training step 784, accuracy: 0.8844\n",
            "After training step 785, accuracy: 0.8816\n",
            "After training step 786, accuracy: 0.8798\n",
            "After training step 787, accuracy: 0.8786\n",
            "After training step 788, accuracy: 0.8780\n",
            "After training step 789, accuracy: 0.8757\n",
            "After training step 790, accuracy: 0.8736\n",
            "After training step 791, accuracy: 0.8726\n",
            "After training step 792, accuracy: 0.8707\n",
            "After training step 793, accuracy: 0.8722\n",
            "After training step 794, accuracy: 0.8712\n",
            "After training step 795, accuracy: 0.8692\n",
            "After training step 796, accuracy: 0.8694\n",
            "After training step 797, accuracy: 0.8697\n",
            "After training step 798, accuracy: 0.8686\n",
            "After training step 799, accuracy: 0.8666\n",
            "After training step 800, accuracy: 0.8636\n",
            "After training step 801, accuracy: 0.8619\n",
            "After training step 802, accuracy: 0.8603\n",
            "After training step 803, accuracy: 0.8593\n",
            "After training step 804, accuracy: 0.8576\n",
            "After training step 805, accuracy: 0.8552\n",
            "After training step 806, accuracy: 0.8511\n",
            "After training step 807, accuracy: 0.8542\n",
            "After training step 808, accuracy: 0.8586\n",
            "After training step 809, accuracy: 0.8642\n",
            "After training step 810, accuracy: 0.8647\n",
            "After training step 811, accuracy: 0.8677\n",
            "After training step 812, accuracy: 0.8719\n",
            "After training step 813, accuracy: 0.8766\n",
            "After training step 814, accuracy: 0.8788\n",
            "After training step 815, accuracy: 0.8782\n",
            "After training step 816, accuracy: 0.8761\n",
            "After training step 817, accuracy: 0.8743\n",
            "After training step 818, accuracy: 0.8733\n",
            "After training step 819, accuracy: 0.8727\n",
            "After training step 820, accuracy: 0.8723\n",
            "After training step 821, accuracy: 0.8737\n",
            "After training step 822, accuracy: 0.8750\n",
            "After training step 823, accuracy: 0.8762\n",
            "After training step 824, accuracy: 0.8786\n",
            "After training step 825, accuracy: 0.8794\n",
            "After training step 826, accuracy: 0.8807\n",
            "After training step 827, accuracy: 0.8825\n",
            "After training step 828, accuracy: 0.8826\n",
            "After training step 829, accuracy: 0.8816\n",
            "After training step 830, accuracy: 0.8837\n",
            "After training step 831, accuracy: 0.8793\n",
            "After training step 832, accuracy: 0.8767\n",
            "After training step 833, accuracy: 0.8726\n",
            "After training step 834, accuracy: 0.8689\n",
            "After training step 835, accuracy: 0.8646\n",
            "After training step 836, accuracy: 0.8609\n",
            "After training step 837, accuracy: 0.8593\n",
            "After training step 838, accuracy: 0.8562\n",
            "After training step 839, accuracy: 0.8563\n",
            "After training step 840, accuracy: 0.8565\n",
            "After training step 841, accuracy: 0.8583\n",
            "After training step 842, accuracy: 0.8589\n",
            "After training step 843, accuracy: 0.8588\n",
            "After training step 844, accuracy: 0.8595\n",
            "After training step 845, accuracy: 0.8585\n",
            "After training step 846, accuracy: 0.8583\n",
            "After training step 847, accuracy: 0.8578\n",
            "After training step 848, accuracy: 0.8564\n",
            "After training step 849, accuracy: 0.8569\n",
            "After training step 850, accuracy: 0.8587\n",
            "After training step 851, accuracy: 0.8602\n",
            "After training step 852, accuracy: 0.8635\n",
            "After training step 853, accuracy: 0.8684\n",
            "After training step 854, accuracy: 0.8732\n",
            "After training step 855, accuracy: 0.8764\n",
            "After training step 856, accuracy: 0.8777\n",
            "After training step 857, accuracy: 0.8754\n",
            "After training step 858, accuracy: 0.8716\n",
            "After training step 859, accuracy: 0.8670\n",
            "After training step 860, accuracy: 0.8599\n",
            "After training step 861, accuracy: 0.8541\n",
            "After training step 862, accuracy: 0.8507\n",
            "After training step 863, accuracy: 0.8489\n",
            "After training step 864, accuracy: 0.8497\n",
            "After training step 865, accuracy: 0.8516\n",
            "After training step 866, accuracy: 0.8559\n",
            "After training step 867, accuracy: 0.8611\n",
            "After training step 868, accuracy: 0.8655\n",
            "After training step 869, accuracy: 0.8692\n",
            "After training step 870, accuracy: 0.8721\n",
            "After training step 871, accuracy: 0.8719\n",
            "After training step 872, accuracy: 0.8710\n",
            "After training step 873, accuracy: 0.8725\n",
            "After training step 874, accuracy: 0.8731\n",
            "After training step 875, accuracy: 0.8748\n",
            "After training step 876, accuracy: 0.8759\n",
            "After training step 877, accuracy: 0.8775\n",
            "After training step 878, accuracy: 0.8783\n",
            "After training step 879, accuracy: 0.8791\n",
            "After training step 880, accuracy: 0.8789\n",
            "After training step 881, accuracy: 0.8780\n",
            "After training step 882, accuracy: 0.8777\n",
            "After training step 883, accuracy: 0.8776\n",
            "After training step 884, accuracy: 0.8768\n",
            "After training step 885, accuracy: 0.8790\n",
            "After training step 886, accuracy: 0.8816\n",
            "After training step 887, accuracy: 0.8842\n",
            "After training step 888, accuracy: 0.8847\n",
            "After training step 889, accuracy: 0.8856\n",
            "After training step 890, accuracy: 0.8842\n",
            "After training step 891, accuracy: 0.8828\n",
            "After training step 892, accuracy: 0.8805\n",
            "After training step 893, accuracy: 0.8789\n",
            "After training step 894, accuracy: 0.8772\n",
            "After training step 895, accuracy: 0.8740\n",
            "After training step 896, accuracy: 0.8709\n",
            "After training step 897, accuracy: 0.8691\n",
            "After training step 898, accuracy: 0.8665\n",
            "After training step 899, accuracy: 0.8646\n",
            "After training step 900, accuracy: 0.8602\n",
            "After training step 901, accuracy: 0.8592\n",
            "After training step 902, accuracy: 0.8594\n",
            "After training step 903, accuracy: 0.8616\n",
            "After training step 904, accuracy: 0.8627\n",
            "After training step 905, accuracy: 0.8616\n",
            "After training step 906, accuracy: 0.8569\n",
            "After training step 907, accuracy: 0.8541\n",
            "After training step 908, accuracy: 0.8520\n",
            "After training step 909, accuracy: 0.8516\n",
            "After training step 910, accuracy: 0.8558\n",
            "After training step 911, accuracy: 0.8626\n",
            "After training step 912, accuracy: 0.8687\n",
            "After training step 913, accuracy: 0.8772\n",
            "After training step 914, accuracy: 0.8819\n",
            "After training step 915, accuracy: 0.8836\n",
            "After training step 916, accuracy: 0.8844\n",
            "After training step 917, accuracy: 0.8856\n",
            "After training step 918, accuracy: 0.8849\n",
            "After training step 919, accuracy: 0.8840\n",
            "After training step 920, accuracy: 0.8839\n",
            "After training step 921, accuracy: 0.8835\n",
            "After training step 922, accuracy: 0.8829\n",
            "After training step 923, accuracy: 0.8795\n",
            "After training step 924, accuracy: 0.8807\n",
            "After training step 925, accuracy: 0.8795\n",
            "After training step 926, accuracy: 0.8778\n",
            "After training step 927, accuracy: 0.8764\n",
            "After training step 928, accuracy: 0.8749\n",
            "After training step 929, accuracy: 0.8733\n",
            "After training step 930, accuracy: 0.8717\n",
            "After training step 931, accuracy: 0.8724\n",
            "After training step 932, accuracy: 0.8714\n",
            "After training step 933, accuracy: 0.8716\n",
            "After training step 934, accuracy: 0.8724\n",
            "After training step 935, accuracy: 0.8721\n",
            "After training step 936, accuracy: 0.8723\n",
            "After training step 937, accuracy: 0.8715\n",
            "After training step 938, accuracy: 0.8723\n",
            "After training step 939, accuracy: 0.8736\n",
            "After training step 940, accuracy: 0.8764\n",
            "After training step 941, accuracy: 0.8772\n",
            "After training step 942, accuracy: 0.8791\n",
            "After training step 943, accuracy: 0.8801\n",
            "After training step 944, accuracy: 0.8816\n",
            "After training step 945, accuracy: 0.8814\n",
            "After training step 946, accuracy: 0.8810\n",
            "After training step 947, accuracy: 0.8810\n",
            "After training step 948, accuracy: 0.8828\n",
            "After training step 949, accuracy: 0.8835\n",
            "After training step 950, accuracy: 0.8812\n",
            "After training step 951, accuracy: 0.8802\n",
            "After training step 952, accuracy: 0.8792\n",
            "After training step 953, accuracy: 0.8772\n",
            "After training step 954, accuracy: 0.8770\n",
            "After training step 955, accuracy: 0.8760\n",
            "After training step 956, accuracy: 0.8755\n",
            "After training step 957, accuracy: 0.8745\n",
            "After training step 958, accuracy: 0.8749\n",
            "After training step 959, accuracy: 0.8754\n",
            "After training step 960, accuracy: 0.8774\n",
            "After training step 961, accuracy: 0.8797\n",
            "After training step 962, accuracy: 0.8811\n",
            "After training step 963, accuracy: 0.8824\n",
            "After training step 964, accuracy: 0.8824\n",
            "After training step 965, accuracy: 0.8837\n",
            "After training step 966, accuracy: 0.8858\n",
            "After training step 967, accuracy: 0.8870\n",
            "After training step 968, accuracy: 0.8873\n",
            "After training step 969, accuracy: 0.8865\n",
            "After training step 970, accuracy: 0.8851\n",
            "After training step 971, accuracy: 0.8833\n",
            "After training step 972, accuracy: 0.8832\n",
            "After training step 973, accuracy: 0.8849\n",
            "After training step 974, accuracy: 0.8851\n",
            "After training step 975, accuracy: 0.8860\n",
            "After training step 976, accuracy: 0.8858\n",
            "After training step 977, accuracy: 0.8853\n",
            "After training step 978, accuracy: 0.8848\n",
            "After training step 979, accuracy: 0.8834\n",
            "After training step 980, accuracy: 0.8820\n",
            "After training step 981, accuracy: 0.8785\n",
            "After training step 982, accuracy: 0.8741\n",
            "After training step 983, accuracy: 0.8711\n",
            "After training step 984, accuracy: 0.8672\n",
            "After training step 985, accuracy: 0.8637\n",
            "After training step 986, accuracy: 0.8618\n",
            "After training step 987, accuracy: 0.8625\n",
            "After training step 988, accuracy: 0.8642\n",
            "After training step 989, accuracy: 0.8671\n",
            "After training step 990, accuracy: 0.8716\n",
            "After training step 991, accuracy: 0.8746\n",
            "After training step 992, accuracy: 0.8772\n",
            "After training step 993, accuracy: 0.8798\n",
            "After training step 994, accuracy: 0.8823\n",
            "After training step 995, accuracy: 0.8816\n",
            "After training step 996, accuracy: 0.8800\n",
            "After training step 997, accuracy: 0.8795\n",
            "After training step 998, accuracy: 0.8786\n",
            "After training step 999, accuracy: 0.8782\n",
            "After training step 1000, accuracy: 0.8772\n",
            "After training step 1001, accuracy: 0.8753\n",
            "After training step 1002, accuracy: 0.8729\n",
            "After training step 1003, accuracy: 0.8723\n",
            "After training step 1004, accuracy: 0.8718\n",
            "After training step 1005, accuracy: 0.8704\n",
            "After training step 1006, accuracy: 0.8722\n",
            "After training step 1007, accuracy: 0.8760\n",
            "After training step 1008, accuracy: 0.8783\n",
            "After training step 1009, accuracy: 0.8806\n",
            "After training step 1010, accuracy: 0.8809\n",
            "After training step 1011, accuracy: 0.8813\n",
            "After training step 1012, accuracy: 0.8827\n",
            "After training step 1013, accuracy: 0.8838\n",
            "After training step 1014, accuracy: 0.8836\n",
            "After training step 1015, accuracy: 0.8850\n",
            "After training step 1016, accuracy: 0.8860\n",
            "After training step 1017, accuracy: 0.8862\n",
            "After training step 1018, accuracy: 0.8859\n",
            "After training step 1019, accuracy: 0.8858\n",
            "After training step 1020, accuracy: 0.8849\n",
            "After training step 1021, accuracy: 0.8836\n",
            "After training step 1022, accuracy: 0.8803\n",
            "After training step 1023, accuracy: 0.8783\n",
            "After training step 1024, accuracy: 0.8754\n",
            "After training step 1025, accuracy: 0.8740\n",
            "After training step 1026, accuracy: 0.8726\n",
            "After training step 1027, accuracy: 0.8721\n",
            "After training step 1028, accuracy: 0.8706\n",
            "After training step 1029, accuracy: 0.8690\n",
            "After training step 1030, accuracy: 0.8690\n",
            "After training step 1031, accuracy: 0.8702\n",
            "After training step 1032, accuracy: 0.8721\n",
            "After training step 1033, accuracy: 0.8746\n",
            "After training step 1034, accuracy: 0.8769\n",
            "After training step 1035, accuracy: 0.8787\n",
            "After training step 1036, accuracy: 0.8787\n",
            "After training step 1037, accuracy: 0.8766\n",
            "After training step 1038, accuracy: 0.8741\n",
            "After training step 1039, accuracy: 0.8721\n",
            "After training step 1040, accuracy: 0.8728\n",
            "After training step 1041, accuracy: 0.8755\n",
            "After training step 1042, accuracy: 0.8789\n",
            "After training step 1043, accuracy: 0.8803\n",
            "After training step 1044, accuracy: 0.8837\n",
            "After training step 1045, accuracy: 0.8868\n",
            "After training step 1046, accuracy: 0.8878\n",
            "After training step 1047, accuracy: 0.8886\n",
            "After training step 1048, accuracy: 0.8884\n",
            "After training step 1049, accuracy: 0.8874\n",
            "After training step 1050, accuracy: 0.8886\n",
            "After training step 1051, accuracy: 0.8891\n",
            "After training step 1052, accuracy: 0.8889\n",
            "After training step 1053, accuracy: 0.8896\n",
            "After training step 1054, accuracy: 0.8894\n",
            "After training step 1055, accuracy: 0.8894\n",
            "After training step 1056, accuracy: 0.8889\n",
            "After training step 1057, accuracy: 0.8860\n",
            "After training step 1058, accuracy: 0.8844\n",
            "After training step 1059, accuracy: 0.8835\n",
            "After training step 1060, accuracy: 0.8853\n",
            "After training step 1061, accuracy: 0.8861\n",
            "After training step 1062, accuracy: 0.8872\n",
            "After training step 1063, accuracy: 0.8876\n",
            "After training step 1064, accuracy: 0.8890\n",
            "After training step 1065, accuracy: 0.8888\n",
            "After training step 1066, accuracy: 0.8872\n",
            "After training step 1067, accuracy: 0.8858\n",
            "After training step 1068, accuracy: 0.8836\n",
            "After training step 1069, accuracy: 0.8804\n",
            "After training step 1070, accuracy: 0.8797\n",
            "After training step 1071, accuracy: 0.8779\n",
            "After training step 1072, accuracy: 0.8761\n",
            "After training step 1073, accuracy: 0.8757\n",
            "After training step 1074, accuracy: 0.8779\n",
            "After training step 1075, accuracy: 0.8792\n",
            "After training step 1076, accuracy: 0.8810\n",
            "After training step 1077, accuracy: 0.8838\n",
            "After training step 1078, accuracy: 0.8852\n",
            "After training step 1079, accuracy: 0.8870\n",
            "After training step 1080, accuracy: 0.8875\n",
            "After training step 1081, accuracy: 0.8883\n",
            "After training step 1082, accuracy: 0.8885\n",
            "After training step 1083, accuracy: 0.8862\n",
            "After training step 1084, accuracy: 0.8854\n",
            "After training step 1085, accuracy: 0.8825\n",
            "After training step 1086, accuracy: 0.8805\n",
            "After training step 1087, accuracy: 0.8791\n",
            "After training step 1088, accuracy: 0.8753\n",
            "After training step 1089, accuracy: 0.8748\n",
            "After training step 1090, accuracy: 0.8749\n",
            "After training step 1091, accuracy: 0.8761\n",
            "After training step 1092, accuracy: 0.8754\n",
            "After training step 1093, accuracy: 0.8749\n",
            "After training step 1094, accuracy: 0.8742\n",
            "After training step 1095, accuracy: 0.8751\n",
            "After training step 1096, accuracy: 0.8756\n",
            "After training step 1097, accuracy: 0.8762\n",
            "After training step 1098, accuracy: 0.8769\n",
            "After training step 1099, accuracy: 0.8801\n",
            "After training step 1100, accuracy: 0.8812\n",
            "After training step 1101, accuracy: 0.8809\n",
            "After training step 1102, accuracy: 0.8812\n",
            "After training step 1103, accuracy: 0.8820\n",
            "After training step 1104, accuracy: 0.8837\n",
            "After training step 1105, accuracy: 0.8830\n",
            "After training step 1106, accuracy: 0.8825\n",
            "After training step 1107, accuracy: 0.8822\n",
            "After training step 1108, accuracy: 0.8796\n",
            "After training step 1109, accuracy: 0.8778\n",
            "After training step 1110, accuracy: 0.8746\n",
            "After training step 1111, accuracy: 0.8726\n",
            "After training step 1112, accuracy: 0.8748\n",
            "After training step 1113, accuracy: 0.8744\n",
            "After training step 1114, accuracy: 0.8745\n",
            "After training step 1115, accuracy: 0.8727\n",
            "After training step 1116, accuracy: 0.8700\n",
            "After training step 1117, accuracy: 0.8679\n",
            "After training step 1118, accuracy: 0.8638\n",
            "After training step 1119, accuracy: 0.8597\n",
            "After training step 1120, accuracy: 0.8560\n",
            "After training step 1121, accuracy: 0.8555\n",
            "After training step 1122, accuracy: 0.8555\n",
            "After training step 1123, accuracy: 0.8546\n",
            "After training step 1124, accuracy: 0.8557\n",
            "After training step 1125, accuracy: 0.8568\n",
            "After training step 1126, accuracy: 0.8606\n",
            "After training step 1127, accuracy: 0.8661\n",
            "After training step 1128, accuracy: 0.8739\n",
            "After training step 1129, accuracy: 0.8789\n",
            "After training step 1130, accuracy: 0.8830\n",
            "After training step 1131, accuracy: 0.8852\n",
            "After training step 1132, accuracy: 0.8856\n",
            "After training step 1133, accuracy: 0.8840\n",
            "After training step 1134, accuracy: 0.8805\n",
            "After training step 1135, accuracy: 0.8784\n",
            "After training step 1136, accuracy: 0.8778\n",
            "After training step 1137, accuracy: 0.8751\n",
            "After training step 1138, accuracy: 0.8723\n",
            "After training step 1139, accuracy: 0.8707\n",
            "After training step 1140, accuracy: 0.8697\n",
            "After training step 1141, accuracy: 0.8700\n",
            "After training step 1142, accuracy: 0.8721\n",
            "After training step 1143, accuracy: 0.8753\n",
            "After training step 1144, accuracy: 0.8767\n",
            "After training step 1145, accuracy: 0.8802\n",
            "After training step 1146, accuracy: 0.8841\n",
            "After training step 1147, accuracy: 0.8876\n",
            "After training step 1148, accuracy: 0.8897\n",
            "After training step 1149, accuracy: 0.8909\n",
            "After training step 1150, accuracy: 0.8910\n",
            "After training step 1151, accuracy: 0.8893\n",
            "After training step 1152, accuracy: 0.8875\n",
            "After training step 1153, accuracy: 0.8843\n",
            "After training step 1154, accuracy: 0.8820\n",
            "After training step 1155, accuracy: 0.8813\n",
            "After training step 1156, accuracy: 0.8811\n",
            "After training step 1157, accuracy: 0.8808\n",
            "After training step 1158, accuracy: 0.8819\n",
            "After training step 1159, accuracy: 0.8830\n",
            "After training step 1160, accuracy: 0.8820\n",
            "After training step 1161, accuracy: 0.8822\n",
            "After training step 1162, accuracy: 0.8822\n",
            "After training step 1163, accuracy: 0.8826\n",
            "After training step 1164, accuracy: 0.8836\n",
            "After training step 1165, accuracy: 0.8838\n",
            "After training step 1166, accuracy: 0.8833\n",
            "After training step 1167, accuracy: 0.8846\n",
            "After training step 1168, accuracy: 0.8857\n",
            "After training step 1169, accuracy: 0.8870\n",
            "After training step 1170, accuracy: 0.8875\n",
            "After training step 1171, accuracy: 0.8879\n",
            "After training step 1172, accuracy: 0.8888\n",
            "After training step 1173, accuracy: 0.8886\n",
            "After training step 1174, accuracy: 0.8875\n",
            "After training step 1175, accuracy: 0.8877\n",
            "After training step 1176, accuracy: 0.8877\n",
            "After training step 1177, accuracy: 0.8883\n",
            "After training step 1178, accuracy: 0.8878\n",
            "After training step 1179, accuracy: 0.8882\n",
            "After training step 1180, accuracy: 0.8892\n",
            "After training step 1181, accuracy: 0.8897\n",
            "After training step 1182, accuracy: 0.8900\n",
            "After training step 1183, accuracy: 0.8895\n",
            "After training step 1184, accuracy: 0.8896\n",
            "After training step 1185, accuracy: 0.8886\n",
            "After training step 1186, accuracy: 0.8893\n",
            "After training step 1187, accuracy: 0.8892\n",
            "After training step 1188, accuracy: 0.8876\n",
            "After training step 1189, accuracy: 0.8871\n",
            "After training step 1190, accuracy: 0.8857\n",
            "After training step 1191, accuracy: 0.8850\n",
            "After training step 1192, accuracy: 0.8855\n",
            "After training step 1193, accuracy: 0.8847\n",
            "After training step 1194, accuracy: 0.8826\n",
            "After training step 1195, accuracy: 0.8831\n",
            "After training step 1196, accuracy: 0.8827\n",
            "After training step 1197, accuracy: 0.8814\n",
            "After training step 1198, accuracy: 0.8807\n",
            "After training step 1199, accuracy: 0.8800\n",
            "After training step 1200, accuracy: 0.8815\n",
            "After training step 1201, accuracy: 0.8815\n",
            "After training step 1202, accuracy: 0.8825\n",
            "After training step 1203, accuracy: 0.8821\n",
            "After training step 1204, accuracy: 0.8831\n",
            "After training step 1205, accuracy: 0.8829\n",
            "After training step 1206, accuracy: 0.8795\n",
            "After training step 1207, accuracy: 0.8757\n",
            "After training step 1208, accuracy: 0.8690\n",
            "After training step 1209, accuracy: 0.8663\n",
            "After training step 1210, accuracy: 0.8645\n",
            "After training step 1211, accuracy: 0.8634\n",
            "After training step 1212, accuracy: 0.8622\n",
            "After training step 1213, accuracy: 0.8637\n",
            "After training step 1214, accuracy: 0.8664\n",
            "After training step 1215, accuracy: 0.8693\n",
            "After training step 1216, accuracy: 0.8734\n",
            "After training step 1217, accuracy: 0.8761\n",
            "After training step 1218, accuracy: 0.8782\n",
            "After training step 1219, accuracy: 0.8825\n",
            "After training step 1220, accuracy: 0.8835\n",
            "After training step 1221, accuracy: 0.8819\n",
            "After training step 1222, accuracy: 0.8824\n",
            "After training step 1223, accuracy: 0.8815\n",
            "After training step 1224, accuracy: 0.8800\n",
            "After training step 1225, accuracy: 0.8793\n",
            "After training step 1226, accuracy: 0.8779\n",
            "After training step 1227, accuracy: 0.8784\n",
            "After training step 1228, accuracy: 0.8771\n",
            "After training step 1229, accuracy: 0.8772\n",
            "After training step 1230, accuracy: 0.8759\n",
            "After training step 1231, accuracy: 0.8768\n",
            "After training step 1232, accuracy: 0.8756\n",
            "After training step 1233, accuracy: 0.8764\n",
            "After training step 1234, accuracy: 0.8768\n",
            "After training step 1235, accuracy: 0.8772\n",
            "After training step 1236, accuracy: 0.8771\n",
            "After training step 1237, accuracy: 0.8770\n",
            "After training step 1238, accuracy: 0.8789\n",
            "After training step 1239, accuracy: 0.8798\n",
            "After training step 1240, accuracy: 0.8814\n",
            "After training step 1241, accuracy: 0.8828\n",
            "After training step 1242, accuracy: 0.8831\n",
            "After training step 1243, accuracy: 0.8840\n",
            "After training step 1244, accuracy: 0.8861\n",
            "After training step 1245, accuracy: 0.8865\n",
            "After training step 1246, accuracy: 0.8876\n",
            "After training step 1247, accuracy: 0.8892\n",
            "After training step 1248, accuracy: 0.8892\n",
            "After training step 1249, accuracy: 0.8894\n",
            "After training step 1250, accuracy: 0.8882\n",
            "After training step 1251, accuracy: 0.8879\n",
            "After training step 1252, accuracy: 0.8867\n",
            "After training step 1253, accuracy: 0.8841\n",
            "After training step 1254, accuracy: 0.8798\n",
            "After training step 1255, accuracy: 0.8753\n",
            "After training step 1256, accuracy: 0.8721\n",
            "After training step 1257, accuracy: 0.8679\n",
            "After training step 1258, accuracy: 0.8620\n",
            "After training step 1259, accuracy: 0.8619\n",
            "After training step 1260, accuracy: 0.8622\n",
            "After training step 1261, accuracy: 0.8636\n",
            "After training step 1262, accuracy: 0.8669\n",
            "After training step 1263, accuracy: 0.8694\n",
            "After training step 1264, accuracy: 0.8745\n",
            "After training step 1265, accuracy: 0.8781\n",
            "After training step 1266, accuracy: 0.8779\n",
            "After training step 1267, accuracy: 0.8761\n",
            "After training step 1268, accuracy: 0.8757\n",
            "After training step 1269, accuracy: 0.8766\n",
            "After training step 1270, accuracy: 0.8793\n",
            "After training step 1271, accuracy: 0.8801\n",
            "After training step 1272, accuracy: 0.8853\n",
            "After training step 1273, accuracy: 0.8880\n",
            "After training step 1274, accuracy: 0.8888\n",
            "After training step 1275, accuracy: 0.8860\n",
            "After training step 1276, accuracy: 0.8852\n",
            "After training step 1277, accuracy: 0.8816\n",
            "After training step 1278, accuracy: 0.8763\n",
            "After training step 1279, accuracy: 0.8734\n",
            "After training step 1280, accuracy: 0.8736\n",
            "After training step 1281, accuracy: 0.8763\n",
            "After training step 1282, accuracy: 0.8794\n",
            "After training step 1283, accuracy: 0.8823\n",
            "After training step 1284, accuracy: 0.8823\n",
            "After training step 1285, accuracy: 0.8840\n",
            "After training step 1286, accuracy: 0.8845\n",
            "After training step 1287, accuracy: 0.8834\n",
            "After training step 1288, accuracy: 0.8795\n",
            "After training step 1289, accuracy: 0.8774\n",
            "After training step 1290, accuracy: 0.8773\n",
            "After training step 1291, accuracy: 0.8742\n",
            "After training step 1292, accuracy: 0.8751\n",
            "After training step 1293, accuracy: 0.8754\n",
            "After training step 1294, accuracy: 0.8810\n",
            "After training step 1295, accuracy: 0.8877\n",
            "After training step 1296, accuracy: 0.8889\n",
            "After training step 1297, accuracy: 0.8875\n",
            "After training step 1298, accuracy: 0.8855\n",
            "After training step 1299, accuracy: 0.8824\n",
            "After training step 1300, accuracy: 0.8784\n",
            "After training step 1301, accuracy: 0.8750\n",
            "After training step 1302, accuracy: 0.8714\n",
            "After training step 1303, accuracy: 0.8677\n",
            "After training step 1304, accuracy: 0.8654\n",
            "After training step 1305, accuracy: 0.8631\n",
            "After training step 1306, accuracy: 0.8653\n",
            "After training step 1307, accuracy: 0.8677\n",
            "After training step 1308, accuracy: 0.8708\n",
            "After training step 1309, accuracy: 0.8748\n",
            "After training step 1310, accuracy: 0.8769\n",
            "After training step 1311, accuracy: 0.8780\n",
            "After training step 1312, accuracy: 0.8815\n",
            "After training step 1313, accuracy: 0.8828\n",
            "After training step 1314, accuracy: 0.8841\n",
            "After training step 1315, accuracy: 0.8844\n",
            "After training step 1316, accuracy: 0.8839\n",
            "After training step 1317, accuracy: 0.8845\n",
            "After training step 1318, accuracy: 0.8860\n",
            "After training step 1319, accuracy: 0.8875\n",
            "After training step 1320, accuracy: 0.8878\n",
            "After training step 1321, accuracy: 0.8874\n",
            "After training step 1322, accuracy: 0.8870\n",
            "After training step 1323, accuracy: 0.8875\n",
            "After training step 1324, accuracy: 0.8865\n",
            "After training step 1325, accuracy: 0.8860\n",
            "After training step 1326, accuracy: 0.8870\n",
            "After training step 1327, accuracy: 0.8855\n",
            "After training step 1328, accuracy: 0.8850\n",
            "After training step 1329, accuracy: 0.8841\n",
            "After training step 1330, accuracy: 0.8820\n",
            "After training step 1331, accuracy: 0.8814\n",
            "After training step 1332, accuracy: 0.8791\n",
            "After training step 1333, accuracy: 0.8761\n",
            "After training step 1334, accuracy: 0.8745\n",
            "After training step 1335, accuracy: 0.8724\n",
            "After training step 1336, accuracy: 0.8721\n",
            "After training step 1337, accuracy: 0.8711\n",
            "After training step 1338, accuracy: 0.8717\n",
            "After training step 1339, accuracy: 0.8705\n",
            "After training step 1340, accuracy: 0.8725\n",
            "After training step 1341, accuracy: 0.8743\n",
            "After training step 1342, accuracy: 0.8766\n",
            "After training step 1343, accuracy: 0.8803\n",
            "After training step 1344, accuracy: 0.8807\n",
            "After training step 1345, accuracy: 0.8798\n",
            "After training step 1346, accuracy: 0.8815\n",
            "After training step 1347, accuracy: 0.8824\n",
            "After training step 1348, accuracy: 0.8835\n",
            "After training step 1349, accuracy: 0.8837\n",
            "After training step 1350, accuracy: 0.8825\n",
            "After training step 1351, accuracy: 0.8842\n",
            "After training step 1352, accuracy: 0.8851\n",
            "After training step 1353, accuracy: 0.8864\n",
            "After training step 1354, accuracy: 0.8875\n",
            "After training step 1355, accuracy: 0.8876\n",
            "After training step 1356, accuracy: 0.8889\n",
            "After training step 1357, accuracy: 0.8883\n",
            "After training step 1358, accuracy: 0.8875\n",
            "After training step 1359, accuracy: 0.8880\n",
            "After training step 1360, accuracy: 0.8884\n",
            "After training step 1361, accuracy: 0.8888\n",
            "After training step 1362, accuracy: 0.8884\n",
            "After training step 1363, accuracy: 0.8889\n",
            "After training step 1364, accuracy: 0.8885\n",
            "After training step 1365, accuracy: 0.8896\n",
            "After training step 1366, accuracy: 0.8894\n",
            "After training step 1367, accuracy: 0.8897\n",
            "After training step 1368, accuracy: 0.8889\n",
            "After training step 1369, accuracy: 0.8900\n",
            "After training step 1370, accuracy: 0.8902\n",
            "After training step 1371, accuracy: 0.8903\n",
            "After training step 1372, accuracy: 0.8903\n",
            "After training step 1373, accuracy: 0.8905\n",
            "After training step 1374, accuracy: 0.8933\n",
            "After training step 1375, accuracy: 0.8927\n",
            "After training step 1376, accuracy: 0.8933\n",
            "After training step 1377, accuracy: 0.8924\n",
            "After training step 1378, accuracy: 0.8923\n",
            "After training step 1379, accuracy: 0.8924\n",
            "After training step 1380, accuracy: 0.8924\n",
            "After training step 1381, accuracy: 0.8916\n",
            "After training step 1382, accuracy: 0.8914\n",
            "After training step 1383, accuracy: 0.8916\n",
            "After training step 1384, accuracy: 0.8929\n",
            "After training step 1385, accuracy: 0.8941\n",
            "After training step 1386, accuracy: 0.8957\n",
            "After training step 1387, accuracy: 0.8956\n",
            "After training step 1388, accuracy: 0.8931\n",
            "After training step 1389, accuracy: 0.8906\n",
            "After training step 1390, accuracy: 0.8872\n",
            "After training step 1391, accuracy: 0.8853\n",
            "After training step 1392, accuracy: 0.8840\n",
            "After training step 1393, accuracy: 0.8817\n",
            "After training step 1394, accuracy: 0.8817\n",
            "After training step 1395, accuracy: 0.8834\n",
            "After training step 1396, accuracy: 0.8863\n",
            "After training step 1397, accuracy: 0.8889\n",
            "After training step 1398, accuracy: 0.8893\n",
            "After training step 1399, accuracy: 0.8906\n",
            "After training step 1400, accuracy: 0.8917\n",
            "After training step 1401, accuracy: 0.8917\n",
            "After training step 1402, accuracy: 0.8928\n",
            "After training step 1403, accuracy: 0.8913\n",
            "After training step 1404, accuracy: 0.8899\n",
            "After training step 1405, accuracy: 0.8876\n",
            "After training step 1406, accuracy: 0.8875\n",
            "After training step 1407, accuracy: 0.8885\n",
            "After training step 1408, accuracy: 0.8895\n",
            "After training step 1409, accuracy: 0.8896\n",
            "After training step 1410, accuracy: 0.8904\n",
            "After training step 1411, accuracy: 0.8901\n",
            "After training step 1412, accuracy: 0.8902\n",
            "After training step 1413, accuracy: 0.8899\n",
            "After training step 1414, accuracy: 0.8899\n",
            "After training step 1415, accuracy: 0.8888\n",
            "After training step 1416, accuracy: 0.8891\n",
            "After training step 1417, accuracy: 0.8864\n",
            "After training step 1418, accuracy: 0.8838\n",
            "After training step 1419, accuracy: 0.8826\n",
            "After training step 1420, accuracy: 0.8815\n",
            "After training step 1421, accuracy: 0.8800\n",
            "After training step 1422, accuracy: 0.8792\n",
            "After training step 1423, accuracy: 0.8784\n",
            "After training step 1424, accuracy: 0.8779\n",
            "After training step 1425, accuracy: 0.8766\n",
            "After training step 1426, accuracy: 0.8764\n",
            "After training step 1427, accuracy: 0.8765\n",
            "After training step 1428, accuracy: 0.8771\n",
            "After training step 1429, accuracy: 0.8781\n",
            "After training step 1430, accuracy: 0.8797\n",
            "After training step 1431, accuracy: 0.8835\n",
            "After training step 1432, accuracy: 0.8846\n",
            "After training step 1433, accuracy: 0.8864\n",
            "After training step 1434, accuracy: 0.8882\n",
            "After training step 1435, accuracy: 0.8898\n",
            "After training step 1436, accuracy: 0.8910\n",
            "After training step 1437, accuracy: 0.8909\n",
            "After training step 1438, accuracy: 0.8902\n",
            "After training step 1439, accuracy: 0.8905\n",
            "After training step 1440, accuracy: 0.8900\n",
            "After training step 1441, accuracy: 0.8909\n",
            "After training step 1442, accuracy: 0.8915\n",
            "After training step 1443, accuracy: 0.8900\n",
            "After training step 1444, accuracy: 0.8851\n",
            "After training step 1445, accuracy: 0.8800\n",
            "After training step 1446, accuracy: 0.8750\n",
            "After training step 1447, accuracy: 0.8740\n",
            "After training step 1448, accuracy: 0.8722\n",
            "After training step 1449, accuracy: 0.8707\n",
            "After training step 1450, accuracy: 0.8712\n",
            "After training step 1451, accuracy: 0.8717\n",
            "After training step 1452, accuracy: 0.8742\n",
            "After training step 1453, accuracy: 0.8774\n",
            "After training step 1454, accuracy: 0.8805\n",
            "After training step 1455, accuracy: 0.8824\n",
            "After training step 1456, accuracy: 0.8834\n",
            "After training step 1457, accuracy: 0.8816\n",
            "After training step 1458, accuracy: 0.8777\n",
            "After training step 1459, accuracy: 0.8772\n",
            "After training step 1460, accuracy: 0.8751\n",
            "After training step 1461, accuracy: 0.8725\n",
            "After training step 1462, accuracy: 0.8724\n",
            "After training step 1463, accuracy: 0.8714\n",
            "After training step 1464, accuracy: 0.8722\n",
            "After training step 1465, accuracy: 0.8727\n",
            "After training step 1466, accuracy: 0.8730\n",
            "After training step 1467, accuracy: 0.8740\n",
            "After training step 1468, accuracy: 0.8740\n",
            "After training step 1469, accuracy: 0.8730\n",
            "After training step 1470, accuracy: 0.8718\n",
            "After training step 1471, accuracy: 0.8704\n",
            "After training step 1472, accuracy: 0.8696\n",
            "After training step 1473, accuracy: 0.8696\n",
            "After training step 1474, accuracy: 0.8711\n",
            "After training step 1475, accuracy: 0.8736\n",
            "After training step 1476, accuracy: 0.8745\n",
            "After training step 1477, accuracy: 0.8780\n",
            "After training step 1478, accuracy: 0.8794\n",
            "After training step 1479, accuracy: 0.8825\n",
            "After training step 1480, accuracy: 0.8842\n",
            "After training step 1481, accuracy: 0.8850\n",
            "After training step 1482, accuracy: 0.8859\n",
            "After training step 1483, accuracy: 0.8867\n",
            "After training step 1484, accuracy: 0.8869\n",
            "After training step 1485, accuracy: 0.8853\n",
            "After training step 1486, accuracy: 0.8854\n",
            "After training step 1487, accuracy: 0.8839\n",
            "After training step 1488, accuracy: 0.8805\n",
            "After training step 1489, accuracy: 0.8788\n",
            "After training step 1490, accuracy: 0.8766\n",
            "After training step 1491, accuracy: 0.8756\n",
            "After training step 1492, accuracy: 0.8735\n",
            "After training step 1493, accuracy: 0.8739\n",
            "After training step 1494, accuracy: 0.8747\n",
            "After training step 1495, accuracy: 0.8768\n",
            "After training step 1496, accuracy: 0.8780\n",
            "After training step 1497, accuracy: 0.8806\n",
            "After training step 1498, accuracy: 0.8815\n",
            "After training step 1499, accuracy: 0.8811\n",
            "After training step 1500, accuracy: 0.8799\n",
            "After training step 1501, accuracy: 0.8771\n",
            "After training step 1502, accuracy: 0.8734\n",
            "After training step 1503, accuracy: 0.8687\n",
            "After training step 1504, accuracy: 0.8666\n",
            "After training step 1505, accuracy: 0.8678\n",
            "After training step 1506, accuracy: 0.8684\n",
            "After training step 1507, accuracy: 0.8731\n",
            "After training step 1508, accuracy: 0.8747\n",
            "After training step 1509, accuracy: 0.8791\n",
            "After training step 1510, accuracy: 0.8823\n",
            "After training step 1511, accuracy: 0.8860\n",
            "After training step 1512, accuracy: 0.8891\n",
            "After training step 1513, accuracy: 0.8916\n",
            "After training step 1514, accuracy: 0.8910\n",
            "After training step 1515, accuracy: 0.8899\n",
            "After training step 1516, accuracy: 0.8875\n",
            "After training step 1517, accuracy: 0.8843\n",
            "After training step 1518, accuracy: 0.8820\n",
            "After training step 1519, accuracy: 0.8822\n",
            "After training step 1520, accuracy: 0.8818\n",
            "After training step 1521, accuracy: 0.8825\n",
            "After training step 1522, accuracy: 0.8847\n",
            "After training step 1523, accuracy: 0.8860\n",
            "After training step 1524, accuracy: 0.8864\n",
            "After training step 1525, accuracy: 0.8859\n",
            "After training step 1526, accuracy: 0.8878\n",
            "After training step 1527, accuracy: 0.8889\n",
            "After training step 1528, accuracy: 0.8887\n",
            "After training step 1529, accuracy: 0.8883\n",
            "After training step 1530, accuracy: 0.8868\n",
            "After training step 1531, accuracy: 0.8864\n",
            "After training step 1532, accuracy: 0.8850\n",
            "After training step 1533, accuracy: 0.8833\n",
            "After training step 1534, accuracy: 0.8807\n",
            "After training step 1535, accuracy: 0.8788\n",
            "After training step 1536, accuracy: 0.8783\n",
            "After training step 1537, accuracy: 0.8777\n",
            "After training step 1538, accuracy: 0.8790\n",
            "After training step 1539, accuracy: 0.8810\n",
            "After training step 1540, accuracy: 0.8826\n",
            "After training step 1541, accuracy: 0.8844\n",
            "After training step 1542, accuracy: 0.8856\n",
            "After training step 1543, accuracy: 0.8866\n",
            "After training step 1544, accuracy: 0.8864\n",
            "After training step 1545, accuracy: 0.8850\n",
            "After training step 1546, accuracy: 0.8842\n",
            "After training step 1547, accuracy: 0.8828\n",
            "After training step 1548, accuracy: 0.8792\n",
            "After training step 1549, accuracy: 0.8783\n",
            "After training step 1550, accuracy: 0.8776\n",
            "After training step 1551, accuracy: 0.8759\n",
            "After training step 1552, accuracy: 0.8745\n",
            "After training step 1553, accuracy: 0.8759\n",
            "After training step 1554, accuracy: 0.8780\n",
            "After training step 1555, accuracy: 0.8788\n",
            "After training step 1556, accuracy: 0.8814\n",
            "After training step 1557, accuracy: 0.8843\n",
            "After training step 1558, accuracy: 0.8866\n",
            "After training step 1559, accuracy: 0.8881\n",
            "After training step 1560, accuracy: 0.8891\n",
            "After training step 1561, accuracy: 0.8901\n",
            "After training step 1562, accuracy: 0.8894\n",
            "After training step 1563, accuracy: 0.8900\n",
            "After training step 1564, accuracy: 0.8911\n",
            "After training step 1565, accuracy: 0.8933\n",
            "After training step 1566, accuracy: 0.8951\n",
            "After training step 1567, accuracy: 0.8949\n",
            "After training step 1568, accuracy: 0.8935\n",
            "After training step 1569, accuracy: 0.8909\n",
            "After training step 1570, accuracy: 0.8887\n",
            "After training step 1571, accuracy: 0.8866\n",
            "After training step 1572, accuracy: 0.8845\n",
            "After training step 1573, accuracy: 0.8809\n",
            "After training step 1574, accuracy: 0.8763\n",
            "After training step 1575, accuracy: 0.8725\n",
            "After training step 1576, accuracy: 0.8695\n",
            "After training step 1577, accuracy: 0.8678\n",
            "After training step 1578, accuracy: 0.8715\n",
            "After training step 1579, accuracy: 0.8754\n",
            "After training step 1580, accuracy: 0.8798\n",
            "After training step 1581, accuracy: 0.8813\n",
            "After training step 1582, accuracy: 0.8841\n",
            "After training step 1583, accuracy: 0.8874\n",
            "After training step 1584, accuracy: 0.8888\n",
            "After training step 1585, accuracy: 0.8897\n",
            "After training step 1586, accuracy: 0.8893\n",
            "After training step 1587, accuracy: 0.8913\n",
            "After training step 1588, accuracy: 0.8910\n",
            "After training step 1589, accuracy: 0.8911\n",
            "After training step 1590, accuracy: 0.8911\n",
            "After training step 1591, accuracy: 0.8916\n",
            "After training step 1592, accuracy: 0.8904\n",
            "After training step 1593, accuracy: 0.8906\n",
            "After training step 1594, accuracy: 0.8906\n",
            "After training step 1595, accuracy: 0.8906\n",
            "After training step 1596, accuracy: 0.8905\n",
            "After training step 1597, accuracy: 0.8915\n",
            "After training step 1598, accuracy: 0.8911\n",
            "After training step 1599, accuracy: 0.8921\n",
            "After training step 1600, accuracy: 0.8921\n",
            "After training step 1601, accuracy: 0.8928\n",
            "After training step 1602, accuracy: 0.8925\n",
            "After training step 1603, accuracy: 0.8904\n",
            "After training step 1604, accuracy: 0.8872\n",
            "After training step 1605, accuracy: 0.8857\n",
            "After training step 1606, accuracy: 0.8854\n",
            "After training step 1607, accuracy: 0.8854\n",
            "After training step 1608, accuracy: 0.8850\n",
            "After training step 1609, accuracy: 0.8835\n",
            "After training step 1610, accuracy: 0.8836\n",
            "After training step 1611, accuracy: 0.8837\n",
            "After training step 1612, accuracy: 0.8849\n",
            "After training step 1613, accuracy: 0.8840\n",
            "After training step 1614, accuracy: 0.8847\n",
            "After training step 1615, accuracy: 0.8868\n",
            "After training step 1616, accuracy: 0.8853\n",
            "After training step 1617, accuracy: 0.8830\n",
            "After training step 1618, accuracy: 0.8820\n",
            "After training step 1619, accuracy: 0.8816\n",
            "After training step 1620, accuracy: 0.8824\n",
            "After training step 1621, accuracy: 0.8854\n",
            "After training step 1622, accuracy: 0.8880\n",
            "After training step 1623, accuracy: 0.8908\n",
            "After training step 1624, accuracy: 0.8928\n",
            "After training step 1625, accuracy: 0.8922\n",
            "After training step 1626, accuracy: 0.8916\n",
            "After training step 1627, accuracy: 0.8916\n",
            "After training step 1628, accuracy: 0.8915\n",
            "After training step 1629, accuracy: 0.8924\n",
            "After training step 1630, accuracy: 0.8915\n",
            "After training step 1631, accuracy: 0.8908\n",
            "After training step 1632, accuracy: 0.8901\n",
            "After training step 1633, accuracy: 0.8895\n",
            "After training step 1634, accuracy: 0.8884\n",
            "After training step 1635, accuracy: 0.8886\n",
            "After training step 1636, accuracy: 0.8882\n",
            "After training step 1637, accuracy: 0.8864\n",
            "After training step 1638, accuracy: 0.8870\n",
            "After training step 1639, accuracy: 0.8860\n",
            "After training step 1640, accuracy: 0.8862\n",
            "After training step 1641, accuracy: 0.8863\n",
            "After training step 1642, accuracy: 0.8867\n",
            "After training step 1643, accuracy: 0.8862\n",
            "After training step 1644, accuracy: 0.8857\n",
            "After training step 1645, accuracy: 0.8848\n",
            "After training step 1646, accuracy: 0.8838\n",
            "After training step 1647, accuracy: 0.8851\n",
            "After training step 1648, accuracy: 0.8869\n",
            "After training step 1649, accuracy: 0.8878\n",
            "After training step 1650, accuracy: 0.8905\n",
            "After training step 1651, accuracy: 0.8913\n",
            "After training step 1652, accuracy: 0.8933\n",
            "After training step 1653, accuracy: 0.8927\n",
            "After training step 1654, accuracy: 0.8931\n",
            "After training step 1655, accuracy: 0.8917\n",
            "After training step 1656, accuracy: 0.8890\n",
            "After training step 1657, accuracy: 0.8865\n",
            "After training step 1658, accuracy: 0.8843\n",
            "After training step 1659, accuracy: 0.8820\n",
            "After training step 1660, accuracy: 0.8821\n",
            "After training step 1661, accuracy: 0.8811\n",
            "After training step 1662, accuracy: 0.8817\n",
            "After training step 1663, accuracy: 0.8821\n",
            "After training step 1664, accuracy: 0.8846\n",
            "After training step 1665, accuracy: 0.8864\n",
            "After training step 1666, accuracy: 0.8872\n",
            "After training step 1667, accuracy: 0.8889\n",
            "After training step 1668, accuracy: 0.8899\n",
            "After training step 1669, accuracy: 0.8902\n",
            "After training step 1670, accuracy: 0.8897\n",
            "After training step 1671, accuracy: 0.8893\n",
            "After training step 1672, accuracy: 0.8876\n",
            "After training step 1673, accuracy: 0.8839\n",
            "After training step 1674, accuracy: 0.8831\n",
            "After training step 1675, accuracy: 0.8820\n",
            "After training step 1676, accuracy: 0.8803\n",
            "After training step 1677, accuracy: 0.8787\n",
            "After training step 1678, accuracy: 0.8779\n",
            "After training step 1679, accuracy: 0.8776\n",
            "After training step 1680, accuracy: 0.8799\n",
            "After training step 1681, accuracy: 0.8833\n",
            "After training step 1682, accuracy: 0.8859\n",
            "After training step 1683, accuracy: 0.8867\n",
            "After training step 1684, accuracy: 0.8907\n",
            "After training step 1685, accuracy: 0.8917\n",
            "After training step 1686, accuracy: 0.8928\n",
            "After training step 1687, accuracy: 0.8914\n",
            "After training step 1688, accuracy: 0.8892\n",
            "After training step 1689, accuracy: 0.8871\n",
            "After training step 1690, accuracy: 0.8837\n",
            "After training step 1691, accuracy: 0.8795\n",
            "After training step 1692, accuracy: 0.8755\n",
            "After training step 1693, accuracy: 0.8741\n",
            "After training step 1694, accuracy: 0.8719\n",
            "After training step 1695, accuracy: 0.8729\n",
            "After training step 1696, accuracy: 0.8760\n",
            "After training step 1697, accuracy: 0.8801\n",
            "After training step 1698, accuracy: 0.8836\n",
            "After training step 1699, accuracy: 0.8845\n",
            "After training step 1700, accuracy: 0.8873\n",
            "After training step 1701, accuracy: 0.8886\n",
            "After training step 1702, accuracy: 0.8892\n",
            "After training step 1703, accuracy: 0.8887\n",
            "After training step 1704, accuracy: 0.8881\n",
            "After training step 1705, accuracy: 0.8866\n",
            "After training step 1706, accuracy: 0.8845\n",
            "After training step 1707, accuracy: 0.8818\n",
            "After training step 1708, accuracy: 0.8801\n",
            "After training step 1709, accuracy: 0.8783\n",
            "After training step 1710, accuracy: 0.8790\n",
            "After training step 1711, accuracy: 0.8803\n",
            "After training step 1712, accuracy: 0.8800\n",
            "After training step 1713, accuracy: 0.8795\n",
            "After training step 1714, accuracy: 0.8802\n",
            "After training step 1715, accuracy: 0.8789\n",
            "After training step 1716, accuracy: 0.8765\n",
            "After training step 1717, accuracy: 0.8790\n",
            "After training step 1718, accuracy: 0.8796\n",
            "After training step 1719, accuracy: 0.8815\n",
            "After training step 1720, accuracy: 0.8834\n",
            "After training step 1721, accuracy: 0.8863\n",
            "After training step 1722, accuracy: 0.8858\n",
            "After training step 1723, accuracy: 0.8865\n",
            "After training step 1724, accuracy: 0.8841\n",
            "After training step 1725, accuracy: 0.8768\n",
            "After training step 1726, accuracy: 0.8672\n",
            "After training step 1727, accuracy: 0.8563\n",
            "After training step 1728, accuracy: 0.8465\n",
            "After training step 1729, accuracy: 0.8353\n",
            "After training step 1730, accuracy: 0.8293\n",
            "After training step 1731, accuracy: 0.8312\n",
            "After training step 1732, accuracy: 0.8346\n",
            "After training step 1733, accuracy: 0.8424\n",
            "After training step 1734, accuracy: 0.8512\n",
            "After training step 1735, accuracy: 0.8569\n",
            "After training step 1736, accuracy: 0.8660\n",
            "After training step 1737, accuracy: 0.8715\n",
            "After training step 1738, accuracy: 0.8731\n",
            "After training step 1739, accuracy: 0.8750\n",
            "After training step 1740, accuracy: 0.8756\n",
            "After training step 1741, accuracy: 0.8751\n",
            "After training step 1742, accuracy: 0.8753\n",
            "After training step 1743, accuracy: 0.8772\n",
            "After training step 1744, accuracy: 0.8781\n",
            "After training step 1745, accuracy: 0.8809\n",
            "After training step 1746, accuracy: 0.8822\n",
            "After training step 1747, accuracy: 0.8841\n",
            "After training step 1748, accuracy: 0.8851\n",
            "After training step 1749, accuracy: 0.8864\n",
            "After training step 1750, accuracy: 0.8867\n",
            "After training step 1751, accuracy: 0.8882\n",
            "After training step 1752, accuracy: 0.8895\n",
            "After training step 1753, accuracy: 0.8890\n",
            "After training step 1754, accuracy: 0.8889\n",
            "After training step 1755, accuracy: 0.8879\n",
            "After training step 1756, accuracy: 0.8881\n",
            "After training step 1757, accuracy: 0.8875\n",
            "After training step 1758, accuracy: 0.8861\n",
            "After training step 1759, accuracy: 0.8848\n",
            "After training step 1760, accuracy: 0.8842\n",
            "After training step 1761, accuracy: 0.8852\n",
            "After training step 1762, accuracy: 0.8853\n",
            "After training step 1763, accuracy: 0.8849\n",
            "After training step 1764, accuracy: 0.8846\n",
            "After training step 1765, accuracy: 0.8832\n",
            "After training step 1766, accuracy: 0.8819\n",
            "After training step 1767, accuracy: 0.8829\n",
            "After training step 1768, accuracy: 0.8825\n",
            "After training step 1769, accuracy: 0.8835\n",
            "After training step 1770, accuracy: 0.8837\n",
            "After training step 1771, accuracy: 0.8823\n",
            "After training step 1772, accuracy: 0.8811\n",
            "After training step 1773, accuracy: 0.8830\n",
            "After training step 1774, accuracy: 0.8831\n",
            "After training step 1775, accuracy: 0.8827\n",
            "After training step 1776, accuracy: 0.8841\n",
            "After training step 1777, accuracy: 0.8850\n",
            "After training step 1778, accuracy: 0.8864\n",
            "After training step 1779, accuracy: 0.8865\n",
            "After training step 1780, accuracy: 0.8876\n",
            "After training step 1781, accuracy: 0.8877\n",
            "After training step 1782, accuracy: 0.8895\n",
            "After training step 1783, accuracy: 0.8894\n",
            "After training step 1784, accuracy: 0.8897\n",
            "After training step 1785, accuracy: 0.8902\n",
            "After training step 1786, accuracy: 0.8904\n",
            "After training step 1787, accuracy: 0.8889\n",
            "After training step 1788, accuracy: 0.8851\n",
            "After training step 1789, accuracy: 0.8825\n",
            "After training step 1790, accuracy: 0.8805\n",
            "After training step 1791, accuracy: 0.8776\n",
            "After training step 1792, accuracy: 0.8769\n",
            "After training step 1793, accuracy: 0.8752\n",
            "After training step 1794, accuracy: 0.8754\n",
            "After training step 1795, accuracy: 0.8761\n",
            "After training step 1796, accuracy: 0.8793\n",
            "After training step 1797, accuracy: 0.8801\n",
            "After training step 1798, accuracy: 0.8834\n",
            "After training step 1799, accuracy: 0.8850\n",
            "After training step 1800, accuracy: 0.8882\n",
            "After training step 1801, accuracy: 0.8896\n",
            "After training step 1802, accuracy: 0.8909\n",
            "After training step 1803, accuracy: 0.8926\n",
            "After training step 1804, accuracy: 0.8920\n",
            "After training step 1805, accuracy: 0.8933\n",
            "After training step 1806, accuracy: 0.8920\n",
            "After training step 1807, accuracy: 0.8909\n",
            "After training step 1808, accuracy: 0.8899\n",
            "After training step 1809, accuracy: 0.8890\n",
            "After training step 1810, accuracy: 0.8878\n",
            "After training step 1811, accuracy: 0.8883\n",
            "After training step 1812, accuracy: 0.8879\n",
            "After training step 1813, accuracy: 0.8882\n",
            "After training step 1814, accuracy: 0.8886\n",
            "After training step 1815, accuracy: 0.8882\n",
            "After training step 1816, accuracy: 0.8883\n",
            "After training step 1817, accuracy: 0.8897\n",
            "After training step 1818, accuracy: 0.8902\n",
            "After training step 1819, accuracy: 0.8903\n",
            "After training step 1820, accuracy: 0.8890\n",
            "After training step 1821, accuracy: 0.8887\n",
            "After training step 1822, accuracy: 0.8892\n",
            "After training step 1823, accuracy: 0.8901\n",
            "After training step 1824, accuracy: 0.8905\n",
            "After training step 1825, accuracy: 0.8914\n",
            "After training step 1826, accuracy: 0.8911\n",
            "After training step 1827, accuracy: 0.8914\n",
            "After training step 1828, accuracy: 0.8889\n",
            "After training step 1829, accuracy: 0.8853\n",
            "After training step 1830, accuracy: 0.8841\n",
            "After training step 1831, accuracy: 0.8843\n",
            "After training step 1832, accuracy: 0.8861\n",
            "After training step 1833, accuracy: 0.8869\n",
            "After training step 1834, accuracy: 0.8885\n",
            "After training step 1835, accuracy: 0.8894\n",
            "After training step 1836, accuracy: 0.8912\n",
            "After training step 1837, accuracy: 0.8920\n",
            "After training step 1838, accuracy: 0.8915\n",
            "After training step 1839, accuracy: 0.8915\n",
            "After training step 1840, accuracy: 0.8896\n",
            "After training step 1841, accuracy: 0.8880\n",
            "After training step 1842, accuracy: 0.8876\n",
            "After training step 1843, accuracy: 0.8862\n",
            "After training step 1844, accuracy: 0.8855\n",
            "After training step 1845, accuracy: 0.8847\n",
            "After training step 1846, accuracy: 0.8840\n",
            "After training step 1847, accuracy: 0.8835\n",
            "After training step 1848, accuracy: 0.8832\n",
            "After training step 1849, accuracy: 0.8839\n",
            "After training step 1850, accuracy: 0.8876\n",
            "After training step 1851, accuracy: 0.8903\n",
            "After training step 1852, accuracy: 0.8908\n",
            "After training step 1853, accuracy: 0.8912\n",
            "After training step 1854, accuracy: 0.8914\n",
            "After training step 1855, accuracy: 0.8899\n",
            "After training step 1856, accuracy: 0.8902\n",
            "After training step 1857, accuracy: 0.8892\n",
            "After training step 1858, accuracy: 0.8879\n",
            "After training step 1859, accuracy: 0.8862\n",
            "After training step 1860, accuracy: 0.8868\n",
            "After training step 1861, accuracy: 0.8869\n",
            "After training step 1862, accuracy: 0.8882\n",
            "After training step 1863, accuracy: 0.8888\n",
            "After training step 1864, accuracy: 0.8899\n",
            "After training step 1865, accuracy: 0.8902\n",
            "After training step 1866, accuracy: 0.8905\n",
            "After training step 1867, accuracy: 0.8902\n",
            "After training step 1868, accuracy: 0.8892\n",
            "After training step 1869, accuracy: 0.8914\n",
            "After training step 1870, accuracy: 0.8906\n",
            "After training step 1871, accuracy: 0.8884\n",
            "After training step 1872, accuracy: 0.8884\n",
            "After training step 1873, accuracy: 0.8882\n",
            "After training step 1874, accuracy: 0.8889\n",
            "After training step 1875, accuracy: 0.8883\n",
            "After training step 1876, accuracy: 0.8880\n",
            "After training step 1877, accuracy: 0.8885\n",
            "After training step 1878, accuracy: 0.8902\n",
            "After training step 1879, accuracy: 0.8903\n",
            "After training step 1880, accuracy: 0.8908\n",
            "After training step 1881, accuracy: 0.8910\n",
            "After training step 1882, accuracy: 0.8909\n",
            "After training step 1883, accuracy: 0.8918\n",
            "After training step 1884, accuracy: 0.8914\n",
            "After training step 1885, accuracy: 0.8918\n",
            "After training step 1886, accuracy: 0.8933\n",
            "After training step 1887, accuracy: 0.8933\n",
            "After training step 1888, accuracy: 0.8935\n",
            "After training step 1889, accuracy: 0.8937\n",
            "After training step 1890, accuracy: 0.8939\n",
            "After training step 1891, accuracy: 0.8941\n",
            "After training step 1892, accuracy: 0.8915\n",
            "After training step 1893, accuracy: 0.8900\n",
            "After training step 1894, accuracy: 0.8875\n",
            "After training step 1895, accuracy: 0.8857\n",
            "After training step 1896, accuracy: 0.8844\n",
            "After training step 1897, accuracy: 0.8843\n",
            "After training step 1898, accuracy: 0.8854\n",
            "After training step 1899, accuracy: 0.8870\n",
            "After training step 1900, accuracy: 0.8879\n",
            "After training step 1901, accuracy: 0.8910\n",
            "After training step 1902, accuracy: 0.8916\n",
            "After training step 1903, accuracy: 0.8945\n",
            "After training step 1904, accuracy: 0.8948\n",
            "After training step 1905, accuracy: 0.8964\n",
            "After training step 1906, accuracy: 0.8969\n",
            "After training step 1907, accuracy: 0.8973\n",
            "After training step 1908, accuracy: 0.8973\n",
            "After training step 1909, accuracy: 0.8974\n",
            "After training step 1910, accuracy: 0.8972\n",
            "After training step 1911, accuracy: 0.8965\n",
            "After training step 1912, accuracy: 0.8971\n",
            "After training step 1913, accuracy: 0.8953\n",
            "After training step 1914, accuracy: 0.8945\n",
            "After training step 1915, accuracy: 0.8945\n",
            "After training step 1916, accuracy: 0.8946\n",
            "After training step 1917, accuracy: 0.8964\n",
            "After training step 1918, accuracy: 0.8987\n",
            "After training step 1919, accuracy: 0.8987\n",
            "After training step 1920, accuracy: 0.8962\n",
            "After training step 1921, accuracy: 0.8946\n",
            "After training step 1922, accuracy: 0.8925\n",
            "After training step 1923, accuracy: 0.8875\n",
            "After training step 1924, accuracy: 0.8845\n",
            "After training step 1925, accuracy: 0.8828\n",
            "After training step 1926, accuracy: 0.8807\n",
            "After training step 1927, accuracy: 0.8803\n",
            "After training step 1928, accuracy: 0.8795\n",
            "After training step 1929, accuracy: 0.8799\n",
            "After training step 1930, accuracy: 0.8809\n",
            "After training step 1931, accuracy: 0.8820\n",
            "After training step 1932, accuracy: 0.8848\n",
            "After training step 1933, accuracy: 0.8875\n",
            "After training step 1934, accuracy: 0.8878\n",
            "After training step 1935, accuracy: 0.8883\n",
            "After training step 1936, accuracy: 0.8888\n",
            "After training step 1937, accuracy: 0.8890\n",
            "After training step 1938, accuracy: 0.8898\n",
            "After training step 1939, accuracy: 0.8894\n",
            "After training step 1940, accuracy: 0.8896\n",
            "After training step 1941, accuracy: 0.8896\n",
            "After training step 1942, accuracy: 0.8891\n",
            "After training step 1943, accuracy: 0.8891\n",
            "After training step 1944, accuracy: 0.8891\n",
            "After training step 1945, accuracy: 0.8893\n",
            "After training step 1946, accuracy: 0.8883\n",
            "After training step 1947, accuracy: 0.8899\n",
            "After training step 1948, accuracy: 0.8922\n",
            "After training step 1949, accuracy: 0.8927\n",
            "After training step 1950, accuracy: 0.8926\n",
            "After training step 1951, accuracy: 0.8923\n",
            "After training step 1952, accuracy: 0.8931\n",
            "After training step 1953, accuracy: 0.8941\n",
            "After training step 1954, accuracy: 0.8951\n",
            "After training step 1955, accuracy: 0.8963\n",
            "After training step 1956, accuracy: 0.8975\n",
            "After training step 1957, accuracy: 0.8974\n",
            "After training step 1958, accuracy: 0.8988\n",
            "After training step 1959, accuracy: 0.8975\n",
            "After training step 1960, accuracy: 0.8968\n",
            "After training step 1961, accuracy: 0.8942\n",
            "After training step 1962, accuracy: 0.8929\n",
            "After training step 1963, accuracy: 0.8920\n",
            "After training step 1964, accuracy: 0.8917\n",
            "After training step 1965, accuracy: 0.8913\n",
            "After training step 1966, accuracy: 0.8912\n",
            "After training step 1967, accuracy: 0.8901\n",
            "After training step 1968, accuracy: 0.8876\n",
            "After training step 1969, accuracy: 0.8867\n",
            "After training step 1970, accuracy: 0.8863\n",
            "After training step 1971, accuracy: 0.8866\n",
            "After training step 1972, accuracy: 0.8850\n",
            "After training step 1973, accuracy: 0.8845\n",
            "After training step 1974, accuracy: 0.8833\n",
            "After training step 1975, accuracy: 0.8830\n",
            "After training step 1976, accuracy: 0.8830\n",
            "After training step 1977, accuracy: 0.8843\n",
            "After training step 1978, accuracy: 0.8839\n",
            "After training step 1979, accuracy: 0.8831\n",
            "After training step 1980, accuracy: 0.8855\n",
            "After training step 1981, accuracy: 0.8872\n",
            "After training step 1982, accuracy: 0.8893\n",
            "After training step 1983, accuracy: 0.8900\n",
            "After training step 1984, accuracy: 0.8888\n",
            "After training step 1985, accuracy: 0.8882\n",
            "After training step 1986, accuracy: 0.8905\n",
            "After training step 1987, accuracy: 0.8899\n",
            "After training step 1988, accuracy: 0.8903\n",
            "After training step 1989, accuracy: 0.8884\n",
            "After training step 1990, accuracy: 0.8868\n",
            "After training step 1991, accuracy: 0.8856\n",
            "After training step 1992, accuracy: 0.8836\n",
            "After training step 1993, accuracy: 0.8847\n",
            "After training step 1994, accuracy: 0.8849\n",
            "After training step 1995, accuracy: 0.8861\n",
            "After training step 1996, accuracy: 0.8862\n",
            "After training step 1997, accuracy: 0.8864\n",
            "After training step 1998, accuracy: 0.8856\n",
            "After training step 1999, accuracy: 0.8855\n",
            "After training step 2000, accuracy: 0.8856\n",
            "After training step 2001, accuracy: 0.8843\n",
            "After training step 2002, accuracy: 0.8826\n",
            "After training step 2003, accuracy: 0.8810\n",
            "After training step 2004, accuracy: 0.8844\n",
            "After training step 2005, accuracy: 0.8883\n",
            "After training step 2006, accuracy: 0.8902\n",
            "After training step 2007, accuracy: 0.8920\n",
            "After training step 2008, accuracy: 0.8933\n",
            "After training step 2009, accuracy: 0.8935\n",
            "After training step 2010, accuracy: 0.8920\n",
            "After training step 2011, accuracy: 0.8924\n",
            "After training step 2012, accuracy: 0.8914\n",
            "After training step 2013, accuracy: 0.8862\n",
            "After training step 2014, accuracy: 0.8800\n",
            "After training step 2015, accuracy: 0.8765\n",
            "After training step 2016, accuracy: 0.8768\n",
            "After training step 2017, accuracy: 0.8758\n",
            "After training step 2018, accuracy: 0.8771\n",
            "After training step 2019, accuracy: 0.8803\n",
            "After training step 2020, accuracy: 0.8845\n",
            "After training step 2021, accuracy: 0.8871\n",
            "After training step 2022, accuracy: 0.8898\n",
            "After training step 2023, accuracy: 0.8924\n",
            "After training step 2024, accuracy: 0.8918\n",
            "After training step 2025, accuracy: 0.8918\n",
            "After training step 2026, accuracy: 0.8917\n",
            "After training step 2027, accuracy: 0.8902\n",
            "After training step 2028, accuracy: 0.8894\n",
            "After training step 2029, accuracy: 0.8892\n",
            "After training step 2030, accuracy: 0.8883\n",
            "After training step 2031, accuracy: 0.8874\n",
            "After training step 2032, accuracy: 0.8849\n",
            "After training step 2033, accuracy: 0.8854\n",
            "After training step 2034, accuracy: 0.8859\n",
            "After training step 2035, accuracy: 0.8837\n",
            "After training step 2036, accuracy: 0.8833\n",
            "After training step 2037, accuracy: 0.8783\n",
            "After training step 2038, accuracy: 0.8759\n",
            "After training step 2039, accuracy: 0.8742\n",
            "After training step 2040, accuracy: 0.8728\n",
            "After training step 2041, accuracy: 0.8718\n",
            "After training step 2042, accuracy: 0.8727\n",
            "After training step 2043, accuracy: 0.8744\n",
            "After training step 2044, accuracy: 0.8762\n",
            "After training step 2045, accuracy: 0.8793\n",
            "After training step 2046, accuracy: 0.8804\n",
            "After training step 2047, accuracy: 0.8845\n",
            "After training step 2048, accuracy: 0.8901\n",
            "After training step 2049, accuracy: 0.8934\n",
            "After training step 2050, accuracy: 0.8954\n",
            "After training step 2051, accuracy: 0.8956\n",
            "After training step 2052, accuracy: 0.8960\n",
            "After training step 2053, accuracy: 0.8956\n",
            "After training step 2054, accuracy: 0.8945\n",
            "After training step 2055, accuracy: 0.8924\n",
            "After training step 2056, accuracy: 0.8910\n",
            "After training step 2057, accuracy: 0.8895\n",
            "After training step 2058, accuracy: 0.8869\n",
            "After training step 2059, accuracy: 0.8849\n",
            "After training step 2060, accuracy: 0.8838\n",
            "After training step 2061, accuracy: 0.8828\n",
            "After training step 2062, accuracy: 0.8809\n",
            "After training step 2063, accuracy: 0.8803\n",
            "After training step 2064, accuracy: 0.8810\n",
            "After training step 2065, accuracy: 0.8800\n",
            "After training step 2066, accuracy: 0.8793\n",
            "After training step 2067, accuracy: 0.8791\n",
            "After training step 2068, accuracy: 0.8794\n",
            "After training step 2069, accuracy: 0.8812\n",
            "After training step 2070, accuracy: 0.8811\n",
            "After training step 2071, accuracy: 0.8822\n",
            "After training step 2072, accuracy: 0.8831\n",
            "After training step 2073, accuracy: 0.8856\n",
            "After training step 2074, accuracy: 0.8862\n",
            "After training step 2075, accuracy: 0.8880\n",
            "After training step 2076, accuracy: 0.8885\n",
            "After training step 2077, accuracy: 0.8896\n",
            "After training step 2078, accuracy: 0.8899\n",
            "After training step 2079, accuracy: 0.8908\n",
            "After training step 2080, accuracy: 0.8921\n",
            "After training step 2081, accuracy: 0.8930\n",
            "After training step 2082, accuracy: 0.8933\n",
            "After training step 2083, accuracy: 0.8925\n",
            "After training step 2084, accuracy: 0.8909\n",
            "After training step 2085, accuracy: 0.8913\n",
            "After training step 2086, accuracy: 0.8907\n",
            "After training step 2087, accuracy: 0.8892\n",
            "After training step 2088, accuracy: 0.8889\n",
            "After training step 2089, accuracy: 0.8881\n",
            "After training step 2090, accuracy: 0.8862\n",
            "After training step 2091, accuracy: 0.8851\n",
            "After training step 2092, accuracy: 0.8840\n",
            "After training step 2093, accuracy: 0.8829\n",
            "After training step 2094, accuracy: 0.8809\n",
            "After training step 2095, accuracy: 0.8805\n",
            "After training step 2096, accuracy: 0.8801\n",
            "After training step 2097, accuracy: 0.8803\n",
            "After training step 2098, accuracy: 0.8810\n",
            "After training step 2099, accuracy: 0.8820\n",
            "After training step 2100, accuracy: 0.8837\n",
            "After training step 2101, accuracy: 0.8872\n",
            "After training step 2102, accuracy: 0.8899\n",
            "After training step 2103, accuracy: 0.8917\n",
            "After training step 2104, accuracy: 0.8925\n",
            "After training step 2105, accuracy: 0.8935\n",
            "After training step 2106, accuracy: 0.8946\n",
            "After training step 2107, accuracy: 0.8946\n",
            "After training step 2108, accuracy: 0.8944\n",
            "After training step 2109, accuracy: 0.8949\n",
            "After training step 2110, accuracy: 0.8941\n",
            "After training step 2111, accuracy: 0.8935\n",
            "After training step 2112, accuracy: 0.8929\n",
            "After training step 2113, accuracy: 0.8918\n",
            "After training step 2114, accuracy: 0.8899\n",
            "After training step 2115, accuracy: 0.8883\n",
            "After training step 2116, accuracy: 0.8877\n",
            "After training step 2117, accuracy: 0.8865\n",
            "After training step 2118, accuracy: 0.8845\n",
            "After training step 2119, accuracy: 0.8824\n",
            "After training step 2120, accuracy: 0.8817\n",
            "After training step 2121, accuracy: 0.8810\n",
            "After training step 2122, accuracy: 0.8796\n",
            "After training step 2123, accuracy: 0.8780\n",
            "After training step 2124, accuracy: 0.8783\n",
            "After training step 2125, accuracy: 0.8814\n",
            "After training step 2126, accuracy: 0.8833\n",
            "After training step 2127, accuracy: 0.8841\n",
            "After training step 2128, accuracy: 0.8857\n",
            "After training step 2129, accuracy: 0.8864\n",
            "After training step 2130, accuracy: 0.8868\n",
            "After training step 2131, accuracy: 0.8881\n",
            "After training step 2132, accuracy: 0.8904\n",
            "After training step 2133, accuracy: 0.8926\n",
            "After training step 2134, accuracy: 0.8929\n",
            "After training step 2135, accuracy: 0.8936\n",
            "After training step 2136, accuracy: 0.8917\n",
            "After training step 2137, accuracy: 0.8916\n",
            "After training step 2138, accuracy: 0.8925\n",
            "After training step 2139, accuracy: 0.8929\n",
            "After training step 2140, accuracy: 0.8928\n",
            "After training step 2141, accuracy: 0.8938\n",
            "After training step 2142, accuracy: 0.8946\n",
            "After training step 2143, accuracy: 0.8941\n",
            "After training step 2144, accuracy: 0.8940\n",
            "After training step 2145, accuracy: 0.8938\n",
            "After training step 2146, accuracy: 0.8927\n",
            "After training step 2147, accuracy: 0.8917\n",
            "After training step 2148, accuracy: 0.8905\n",
            "After training step 2149, accuracy: 0.8898\n",
            "After training step 2150, accuracy: 0.8864\n",
            "After training step 2151, accuracy: 0.8856\n",
            "After training step 2152, accuracy: 0.8853\n",
            "After training step 2153, accuracy: 0.8842\n",
            "After training step 2154, accuracy: 0.8836\n",
            "After training step 2155, accuracy: 0.8842\n",
            "After training step 2156, accuracy: 0.8842\n",
            "After training step 2157, accuracy: 0.8847\n",
            "After training step 2158, accuracy: 0.8860\n",
            "After training step 2159, accuracy: 0.8881\n",
            "After training step 2160, accuracy: 0.8881\n",
            "After training step 2161, accuracy: 0.8864\n",
            "After training step 2162, accuracy: 0.8867\n",
            "After training step 2163, accuracy: 0.8871\n",
            "After training step 2164, accuracy: 0.8890\n",
            "After training step 2165, accuracy: 0.8907\n",
            "After training step 2166, accuracy: 0.8920\n",
            "After training step 2167, accuracy: 0.8921\n",
            "After training step 2168, accuracy: 0.8920\n",
            "After training step 2169, accuracy: 0.8916\n",
            "After training step 2170, accuracy: 0.8918\n",
            "After training step 2171, accuracy: 0.8930\n",
            "After training step 2172, accuracy: 0.8938\n",
            "After training step 2173, accuracy: 0.8953\n",
            "After training step 2174, accuracy: 0.8953\n",
            "After training step 2175, accuracy: 0.8958\n",
            "After training step 2176, accuracy: 0.8986\n",
            "After training step 2177, accuracy: 0.8967\n",
            "After training step 2178, accuracy: 0.8964\n",
            "After training step 2179, accuracy: 0.8960\n",
            "After training step 2180, accuracy: 0.8948\n",
            "After training step 2181, accuracy: 0.8952\n",
            "After training step 2182, accuracy: 0.8945\n",
            "After training step 2183, accuracy: 0.8939\n",
            "After training step 2184, accuracy: 0.8939\n",
            "After training step 2185, accuracy: 0.8944\n",
            "After training step 2186, accuracy: 0.8934\n",
            "After training step 2187, accuracy: 0.8936\n",
            "After training step 2188, accuracy: 0.8929\n",
            "After training step 2189, accuracy: 0.8936\n",
            "After training step 2190, accuracy: 0.8925\n",
            "After training step 2191, accuracy: 0.8928\n",
            "After training step 2192, accuracy: 0.8916\n",
            "After training step 2193, accuracy: 0.8919\n",
            "After training step 2194, accuracy: 0.8924\n",
            "After training step 2195, accuracy: 0.8926\n",
            "After training step 2196, accuracy: 0.8913\n",
            "After training step 2197, accuracy: 0.8916\n",
            "After training step 2198, accuracy: 0.8917\n",
            "After training step 2199, accuracy: 0.8923\n",
            "After training step 2200, accuracy: 0.8921\n",
            "After training step 2201, accuracy: 0.8927\n",
            "After training step 2202, accuracy: 0.8924\n",
            "After training step 2203, accuracy: 0.8928\n",
            "After training step 2204, accuracy: 0.8939\n",
            "After training step 2205, accuracy: 0.8948\n",
            "After training step 2206, accuracy: 0.8938\n",
            "After training step 2207, accuracy: 0.8932\n",
            "After training step 2208, accuracy: 0.8941\n",
            "After training step 2209, accuracy: 0.8940\n",
            "After training step 2210, accuracy: 0.8927\n",
            "After training step 2211, accuracy: 0.8929\n",
            "After training step 2212, accuracy: 0.8909\n",
            "After training step 2213, accuracy: 0.8913\n",
            "After training step 2214, accuracy: 0.8910\n",
            "After training step 2215, accuracy: 0.8917\n",
            "After training step 2216, accuracy: 0.8916\n",
            "After training step 2217, accuracy: 0.8906\n",
            "After training step 2218, accuracy: 0.8907\n",
            "After training step 2219, accuracy: 0.8914\n",
            "After training step 2220, accuracy: 0.8890\n",
            "After training step 2221, accuracy: 0.8884\n",
            "After training step 2222, accuracy: 0.8870\n",
            "After training step 2223, accuracy: 0.8867\n",
            "After training step 2224, accuracy: 0.8872\n",
            "After training step 2225, accuracy: 0.8882\n",
            "After training step 2226, accuracy: 0.8889\n",
            "After training step 2227, accuracy: 0.8878\n",
            "After training step 2228, accuracy: 0.8869\n",
            "After training step 2229, accuracy: 0.8854\n",
            "After training step 2230, accuracy: 0.8836\n",
            "After training step 2231, accuracy: 0.8823\n",
            "After training step 2232, accuracy: 0.8800\n",
            "After training step 2233, accuracy: 0.8804\n",
            "After training step 2234, accuracy: 0.8801\n",
            "After training step 2235, accuracy: 0.8831\n",
            "After training step 2236, accuracy: 0.8865\n",
            "After training step 2237, accuracy: 0.8894\n",
            "After training step 2238, accuracy: 0.8912\n",
            "After training step 2239, accuracy: 0.8938\n",
            "After training step 2240, accuracy: 0.8922\n",
            "After training step 2241, accuracy: 0.8892\n",
            "After training step 2242, accuracy: 0.8889\n",
            "After training step 2243, accuracy: 0.8869\n",
            "After training step 2244, accuracy: 0.8835\n",
            "After training step 2245, accuracy: 0.8834\n",
            "After training step 2246, accuracy: 0.8853\n",
            "After training step 2247, accuracy: 0.8881\n",
            "After training step 2248, accuracy: 0.8898\n",
            "After training step 2249, accuracy: 0.8914\n",
            "After training step 2250, accuracy: 0.8913\n",
            "After training step 2251, accuracy: 0.8910\n",
            "After training step 2252, accuracy: 0.8913\n",
            "After training step 2253, accuracy: 0.8914\n",
            "After training step 2254, accuracy: 0.8910\n",
            "After training step 2255, accuracy: 0.8924\n",
            "After training step 2256, accuracy: 0.8945\n",
            "After training step 2257, accuracy: 0.8947\n",
            "After training step 2258, accuracy: 0.8951\n",
            "After training step 2259, accuracy: 0.8952\n",
            "After training step 2260, accuracy: 0.8941\n",
            "After training step 2261, accuracy: 0.8925\n",
            "After training step 2262, accuracy: 0.8909\n",
            "After training step 2263, accuracy: 0.8898\n",
            "After training step 2264, accuracy: 0.8899\n",
            "After training step 2265, accuracy: 0.8905\n",
            "After training step 2266, accuracy: 0.8908\n",
            "After training step 2267, accuracy: 0.8917\n",
            "After training step 2268, accuracy: 0.8922\n",
            "After training step 2269, accuracy: 0.8919\n",
            "After training step 2270, accuracy: 0.8915\n",
            "After training step 2271, accuracy: 0.8903\n",
            "After training step 2272, accuracy: 0.8900\n",
            "After training step 2273, accuracy: 0.8886\n",
            "After training step 2274, accuracy: 0.8890\n",
            "After training step 2275, accuracy: 0.8887\n",
            "After training step 2276, accuracy: 0.8899\n",
            "After training step 2277, accuracy: 0.8916\n",
            "After training step 2278, accuracy: 0.8934\n",
            "After training step 2279, accuracy: 0.8942\n",
            "After training step 2280, accuracy: 0.8952\n",
            "After training step 2281, accuracy: 0.8962\n",
            "After training step 2282, accuracy: 0.8953\n",
            "After training step 2283, accuracy: 0.8957\n",
            "After training step 2284, accuracy: 0.8950\n",
            "After training step 2285, accuracy: 0.8924\n",
            "After training step 2286, accuracy: 0.8905\n",
            "After training step 2287, accuracy: 0.8891\n",
            "After training step 2288, accuracy: 0.8877\n",
            "After training step 2289, accuracy: 0.8869\n",
            "After training step 2290, accuracy: 0.8854\n",
            "After training step 2291, accuracy: 0.8844\n",
            "After training step 2292, accuracy: 0.8839\n",
            "After training step 2293, accuracy: 0.8821\n",
            "After training step 2294, accuracy: 0.8812\n",
            "After training step 2295, accuracy: 0.8799\n",
            "After training step 2296, accuracy: 0.8817\n",
            "After training step 2297, accuracy: 0.8834\n",
            "After training step 2298, accuracy: 0.8819\n",
            "After training step 2299, accuracy: 0.8819\n",
            "After training step 2300, accuracy: 0.8836\n",
            "After training step 2301, accuracy: 0.8869\n",
            "After training step 2302, accuracy: 0.8882\n",
            "After training step 2303, accuracy: 0.8891\n",
            "After training step 2304, accuracy: 0.8907\n",
            "After training step 2305, accuracy: 0.8926\n",
            "After training step 2306, accuracy: 0.8927\n",
            "After training step 2307, accuracy: 0.8919\n",
            "After training step 2308, accuracy: 0.8905\n",
            "After training step 2309, accuracy: 0.8898\n",
            "After training step 2310, accuracy: 0.8879\n",
            "After training step 2311, accuracy: 0.8869\n",
            "After training step 2312, accuracy: 0.8877\n",
            "After training step 2313, accuracy: 0.8893\n",
            "After training step 2314, accuracy: 0.8891\n",
            "After training step 2315, accuracy: 0.8895\n",
            "After training step 2316, accuracy: 0.8904\n",
            "After training step 2317, accuracy: 0.8910\n",
            "After training step 2318, accuracy: 0.8899\n",
            "After training step 2319, accuracy: 0.8880\n",
            "After training step 2320, accuracy: 0.8869\n",
            "After training step 2321, accuracy: 0.8869\n",
            "After training step 2322, accuracy: 0.8878\n",
            "After training step 2323, accuracy: 0.8879\n",
            "After training step 2324, accuracy: 0.8867\n",
            "After training step 2325, accuracy: 0.8875\n",
            "After training step 2326, accuracy: 0.8887\n",
            "After training step 2327, accuracy: 0.8903\n",
            "After training step 2328, accuracy: 0.8912\n",
            "After training step 2329, accuracy: 0.8914\n",
            "After training step 2330, accuracy: 0.8925\n",
            "After training step 2331, accuracy: 0.8934\n",
            "After training step 2332, accuracy: 0.8949\n",
            "After training step 2333, accuracy: 0.8965\n",
            "After training step 2334, accuracy: 0.8970\n",
            "After training step 2335, accuracy: 0.8977\n",
            "After training step 2336, accuracy: 0.8960\n",
            "After training step 2337, accuracy: 0.8956\n",
            "After training step 2338, accuracy: 0.8930\n",
            "After training step 2339, accuracy: 0.8921\n",
            "After training step 2340, accuracy: 0.8893\n",
            "After training step 2341, accuracy: 0.8873\n",
            "After training step 2342, accuracy: 0.8830\n",
            "After training step 2343, accuracy: 0.8807\n",
            "After training step 2344, accuracy: 0.8798\n",
            "After training step 2345, accuracy: 0.8816\n",
            "After training step 2346, accuracy: 0.8826\n",
            "After training step 2347, accuracy: 0.8831\n",
            "After training step 2348, accuracy: 0.8841\n",
            "After training step 2349, accuracy: 0.8857\n",
            "After training step 2350, accuracy: 0.8857\n",
            "After training step 2351, accuracy: 0.8847\n",
            "After training step 2352, accuracy: 0.8850\n",
            "After training step 2353, accuracy: 0.8851\n",
            "After training step 2354, accuracy: 0.8857\n",
            "After training step 2355, accuracy: 0.8859\n",
            "After training step 2356, accuracy: 0.8863\n",
            "After training step 2357, accuracy: 0.8865\n",
            "After training step 2358, accuracy: 0.8870\n",
            "After training step 2359, accuracy: 0.8881\n",
            "After training step 2360, accuracy: 0.8890\n",
            "After training step 2361, accuracy: 0.8906\n",
            "After training step 2362, accuracy: 0.8917\n",
            "After training step 2363, accuracy: 0.8925\n",
            "After training step 2364, accuracy: 0.8925\n",
            "After training step 2365, accuracy: 0.8928\n",
            "After training step 2366, accuracy: 0.8921\n",
            "After training step 2367, accuracy: 0.8921\n",
            "After training step 2368, accuracy: 0.8913\n",
            "After training step 2369, accuracy: 0.8907\n",
            "After training step 2370, accuracy: 0.8901\n",
            "After training step 2371, accuracy: 0.8895\n",
            "After training step 2372, accuracy: 0.8900\n",
            "After training step 2373, accuracy: 0.8909\n",
            "After training step 2374, accuracy: 0.8916\n",
            "After training step 2375, accuracy: 0.8932\n",
            "After training step 2376, accuracy: 0.8934\n",
            "After training step 2377, accuracy: 0.8932\n",
            "After training step 2378, accuracy: 0.8923\n",
            "After training step 2379, accuracy: 0.8902\n",
            "After training step 2380, accuracy: 0.8889\n",
            "After training step 2381, accuracy: 0.8867\n",
            "After training step 2382, accuracy: 0.8853\n",
            "After training step 2383, accuracy: 0.8856\n",
            "After training step 2384, accuracy: 0.8861\n",
            "After training step 2385, accuracy: 0.8878\n",
            "After training step 2386, accuracy: 0.8886\n",
            "After training step 2387, accuracy: 0.8898\n",
            "After training step 2388, accuracy: 0.8896\n",
            "After training step 2389, accuracy: 0.8899\n",
            "After training step 2390, accuracy: 0.8906\n",
            "After training step 2391, accuracy: 0.8892\n",
            "After training step 2392, accuracy: 0.8883\n",
            "After training step 2393, accuracy: 0.8880\n",
            "After training step 2394, accuracy: 0.8881\n",
            "After training step 2395, accuracy: 0.8894\n",
            "After training step 2396, accuracy: 0.8902\n",
            "After training step 2397, accuracy: 0.8895\n",
            "After training step 2398, accuracy: 0.8900\n",
            "After training step 2399, accuracy: 0.8890\n",
            "After training step 2400, accuracy: 0.8874\n"
          ]
        }
      ]
    }
  ]
}